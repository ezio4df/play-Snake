{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Snake 10x10",
   "id": "3c29c28ed1582b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:52.928481791Z",
     "start_time": "2026-01-01T07:32:52.885979122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import gym_snakegame\n",
    "from scipy.signal import step\n",
    "import pygame\n",
    "import sys\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Pytorch device:\", device)"
   ],
   "id": "89bb63c31604d604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch device: cuda\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the gymnasium env",
   "id": "50f2a5f052929594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:29:59.641169603Z",
     "start_time": "2026-01-01T07:29:41.654526595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PLAY_MANUAL = True  # set to False if u wanna see random moves\n",
    "env = gym.make(\n",
    "    \"gym_snakegame/SnakeGame-v0\",\n",
    "    board_size=5,\n",
    "    n_channel=1,\n",
    "    n_target=1,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "KEY_MAP = {\n",
    "    pygame.K_UP: 2,\n",
    "    pygame.K_RIGHT: 1,\n",
    "    pygame.K_DOWN: 0,\n",
    "    pygame.K_LEFT: 3\n",
    "}\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"hit arrows to play! or close window to quit\")\n",
    "if not PLAY_MANUAL:\n",
    "    print(\"auto mode: doing 250 random steps...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        action = None\n",
    "\n",
    "        if PLAY_MANUAL:\n",
    "            # wait for key press\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    env.close()\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key in KEY_MAP:\n",
    "                        action = KEY_MAP[event.key]\n",
    "            if action is None:\n",
    "                continue  # wait till key pressed\n",
    "        else:\n",
    "            if steps >= 250:\n",
    "                break\n",
    "            action = env.action_space.sample()\n",
    "            steps += 1\n",
    "\n",
    "        # step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        print(f\"step {steps}/100, score: {total_reward} ({reward} | trunc:{truncated} | trmt:{terminated}) | {info} | {obs}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"GameOver!  total score: {total_reward}\")\n",
    "            # reset everything\n",
    "            obs, info = env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ],
   "id": "99932bac4b3f83ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit arrows to play! or close window to quit\n",
      "step 0/100, score: 0 (0 | trunc:False | trmt:False) | {'snake_length': 3, 'prev_action': 1} | [[[ 0  0  0  0  0]\n",
      "  [26  0  1  0  0]\n",
      "  [ 0  3  2  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 0 (0 | trunc:False | trmt:False) | {'snake_length': 3, 'prev_action': 2} | [[[ 0  0  0  0  0]\n",
      "  [26  1  2  0  0]\n",
      "  [ 0  0  3  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (1 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 1  2  3  0  0]\n",
      "  [ 0  0  4  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 2  3  4  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 3  4  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 4  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 1 26  0  0  0]]]\n",
      "step 0/100, score: 2 (1 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 4  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 2  1  0  0  0]]]\n",
      "step 0/100, score: 2 (0 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 1} | [[[ 0 26  0  0  0]\n",
      "  [ 0 26  0  0  0]\n",
      "  [ 5  0  0  0  0]\n",
      "  [ 4  1  0  0  0]\n",
      "  [ 3  2  0  0  0]]]\n",
      "step 0/100, score: 2 (0 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0 26  0  0  0]\n",
      "  [ 0  1  0  0  0]\n",
      "  [ 5  2  0  0  0]\n",
      "  [ 4  3  0  0  0]]]\n",
      "step 0/100, score: 3 (1 | trunc:False | trmt:False) | {'snake_length': 6, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0  1  0  0  0]\n",
      "  [ 0  2 26  0  0]\n",
      "  [ 6  3  0  0  0]\n",
      "  [ 5  4  0  0  0]]]\n",
      "step 0/100, score: 4 (1 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 2} | [[[ 0  1  0  0  0]\n",
      "  [ 0  2  0  0  0]\n",
      "  [26  3 26  0  0]\n",
      "  [ 7  4  0  0  0]\n",
      "  [ 6  5  0  0  0]]]\n",
      "step 0/100, score: 4 (0 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 2} | [[[ 0  2  1  0  0]\n",
      "  [ 0  3  0  0  0]\n",
      "  [26  4 26  0  0]\n",
      "  [ 0  5  0  0  0]\n",
      "  [ 7  6  0  0  0]]]\n",
      "step 0/100, score: 4 (0 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 1} | [[[ 0  3  2  0  0]\n",
      "  [ 0  4  1  0  0]\n",
      "  [26  5 26  0  0]\n",
      "  [ 0  6  0  0  0]\n",
      "  [ 0  7  0  0  0]]]\n",
      "step 0/100, score: 5 (1 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  4  3  0 26]\n",
      "  [ 0  5  2  0  0]\n",
      "  [26  6  1  0  0]\n",
      "  [ 0  7  0  0  0]\n",
      "  [ 0  8  0  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  5  4  0 26]\n",
      "  [ 0  6  3  0  0]\n",
      "  [26  7  2  0  0]\n",
      "  [ 0  8  1  0  0]\n",
      "  [ 0  0  0  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  6  5  0 26]\n",
      "  [ 0  7  4  0  0]\n",
      "  [26  8  3  0  0]\n",
      "  [ 0  0  2  0  0]\n",
      "  [ 0  0  1  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  7  6  0 26]\n",
      "  [ 0  8  5  0  0]\n",
      "  [26  0  4  0  0]\n",
      "  [ 0  0  3  0  0]\n",
      "  [ 0  0  2  1  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 1} | [[[ 0  8  7  0 26]\n",
      "  [ 0  0  6  0  0]\n",
      "  [26  0  5  0  0]\n",
      "  [ 0  0  4  1  0]\n",
      "  [ 0  0  3  2  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  8  0 26]\n",
      "  [ 0  0  7  0  0]\n",
      "  [26  0  6  1  0]\n",
      "  [ 0  0  5  2  0]\n",
      "  [ 0  0  4  3  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  0  0 26]\n",
      "  [ 0  0  8  1  0]\n",
      "  [26  0  7  2  0]\n",
      "  [ 0  0  6  3  0]\n",
      "  [ 0  0  5  4  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  0  1 26]\n",
      "  [ 0  0  0  2  0]\n",
      "  [26  0  8  3  0]\n",
      "  [ 0  0  7  4  0]\n",
      "  [ 0  0  6  5  0]]]\n",
      "step 0/100, score: 6 (1 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 2} | [[[ 0 26  0  2  1]\n",
      "  [ 0  0  0  3  0]\n",
      "  [26  0  9  4  0]\n",
      "  [ 0  0  8  5  0]\n",
      "  [ 0  0  7  6  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 1} | [[[ 0 26  0  3  2]\n",
      "  [ 0  0  0  4  1]\n",
      "  [26  0  0  5  0]\n",
      "  [ 0  0  9  6  0]\n",
      "  [ 0  0  8  7  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  4  3]\n",
      "  [ 0  0  0  5  2]\n",
      "  [26  0  0  6  1]\n",
      "  [ 0  0  0  7  0]\n",
      "  [ 0  0  9  8  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  5  4]\n",
      "  [ 0  0  0  6  3]\n",
      "  [26  0  0  7  2]\n",
      "  [ 0  0  0  8  1]\n",
      "  [ 0  0  0  9  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  6  5]\n",
      "  [ 0  0  0  7  4]\n",
      "  [26  0  0  8  3]\n",
      "  [ 0  0  0  9  2]\n",
      "  [ 0  0  0  0  1]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  7  6]\n",
      "  [ 0  0  0  8  5]\n",
      "  [26  0  0  9  4]\n",
      "  [ 0  0  0  0  3]\n",
      "  [ 0  0  0  1  2]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  8  7]\n",
      "  [ 0  0  0  9  6]\n",
      "  [26  0  0  0  5]\n",
      "  [ 0  0  0  0  4]\n",
      "  [ 0  0  1  2  3]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  9  8]\n",
      "  [ 0  0  0  0  7]\n",
      "  [26  0  0  0  6]\n",
      "  [ 0  0  0  0  5]\n",
      "  [ 0  1  2  3  4]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  0  9]\n",
      "  [ 0  0  0  0  8]\n",
      "  [26  0  0  0  7]\n",
      "  [ 0  0  0  0  6]\n",
      "  [ 1  2  3  4  5]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 0  0  0  0  9]\n",
      "  [26  0  0  0  8]\n",
      "  [ 1  0  0  0  7]\n",
      "  [ 2  3  4  5  6]]]\n",
      "step 0/100, score: 7 (1 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0  0  0  0 10]\n",
      "  [ 1  0  0  0  9]\n",
      "  [ 2  0  0 26  8]\n",
      "  [ 3  4  5  6  7]]]\n",
      "step 0/100, score: 7 (0 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 2  0  0  0 10]\n",
      "  [ 3  0  0 26  9]\n",
      "  [ 4  5  6  7  8]]]\n",
      "step 0/100, score: 7 (0 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 1 26  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 4  0  0 26 10]\n",
      "  [ 5  6  7  8  9]]]\n",
      "step 0/100, score: 8 (1 | trunc:False | trmt:False) | {'snake_length': 11, 'prev_action': 2} | [[[ 2  1  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 4 26  0  0  0]\n",
      "  [ 5  0  0 26 11]\n",
      "  [ 6  7  8  9 10]]]\n",
      "step 0/100, score: 8 (0 | trunc:False | trmt:False) | {'snake_length': 11, 'prev_action': 1} | [[[ 3  2  0  0  0]\n",
      "  [ 4  1  0  0  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 6  0  0 26  0]\n",
      "  [ 7  8  9 10 11]]]\n",
      "step 0/100, score: 9 (1 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 4  3  0  0  0]\n",
      "  [ 5  2  0  0 26]\n",
      "  [ 6  1  0  0  0]\n",
      "  [ 7  0  0 26  0]\n",
      "  [ 8  9 10 11 12]]]\n",
      "step 0/100, score: 9 (0 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 5  4  0  0  0]\n",
      "  [ 6  3  0  0 26]\n",
      "  [ 7  2  0  0  0]\n",
      "  [ 8  1  0 26  0]\n",
      "  [ 9 10 11 12  0]]]\n",
      "step 0/100, score: 9 (0 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 6  5  0  0  0]\n",
      "  [ 7  4  0  0 26]\n",
      "  [ 8  3  0  0  0]\n",
      "  [ 9  2  1 26  0]\n",
      "  [10 11 12  0  0]]]\n",
      "step 0/100, score: 10 (1 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[ 7  6 26  0  0]\n",
      "  [ 8  5  0  0 26]\n",
      "  [ 9  4  0  0  0]\n",
      "  [10  3  2  1  0]\n",
      "  [11 12 13  0  0]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[ 8  7 26  0  0]\n",
      "  [ 9  6  0  0 26]\n",
      "  [10  5  0  0  0]\n",
      "  [11  4  3  2  0]\n",
      "  [12 13  0  1  0]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 0} | [[[ 9  8 26  0  0]\n",
      "  [10  7  0  0 26]\n",
      "  [11  6  0  0  0]\n",
      "  [12  5  4  3  0]\n",
      "  [13  0  0  2  1]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[10  9 26  0  0]\n",
      "  [11  8  0  0 26]\n",
      "  [12  7  0  0  0]\n",
      "  [13  6  5  4  1]\n",
      "  [ 0  0  0  3  2]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 2} | [[[11 10 26  0  0]\n",
      "  [12  9  0  0 26]\n",
      "  [13  8  0  0  1]\n",
      "  [ 0  7  6  5  2]\n",
      "  [ 0  0  0  4  3]]]\n",
      "step 0/100, score: 11 (1 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[12 11 26  0  0]\n",
      "  [13 10  0  0  1]\n",
      "  [14  9  0  0  2]\n",
      "  [ 0  8  7  6  3]\n",
      "  [26  0  0  5  4]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[13 12 26  0  1]\n",
      "  [14 11  0  0  2]\n",
      "  [ 0 10  0  0  3]\n",
      "  [ 0  9  8  7  4]\n",
      "  [26  0  0  6  5]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[14 13 26  1  2]\n",
      "  [ 0 12  0  0  3]\n",
      "  [ 0 11  0  0  4]\n",
      "  [ 0 10  9  8  5]\n",
      "  [26  0  0  7  6]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 3} | [[[ 0 14 26  2  3]\n",
      "  [ 0 13  0  1  4]\n",
      "  [ 0 12  0  0  5]\n",
      "  [ 0 11 10  9  6]\n",
      "  [26  0  0  8  7]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 0} | [[[ 0  0 26  3  4]\n",
      "  [ 0 14  0  2  5]\n",
      "  [ 0 13  0  1  6]\n",
      "  [ 0 12 11 10  7]\n",
      "  [26  0  0  9  8]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 0} | [[[ 0  0 26  4  5]\n",
      "  [ 0  0  0  3  6]\n",
      "  [ 0 14  1  2  7]\n",
      "  [ 0 13 12 11  8]\n",
      "  [26  0  0 10  9]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 3} | [[[ 0  0 26  5  6]\n",
      "  [ 0  0  1  4  7]\n",
      "  [ 0  0  2  3  8]\n",
      "  [ 0 14 13 12  9]\n",
      "  [26  0  0 11 10]]]\n",
      "step 0/100, score: 12 (1 | trunc:False | trmt:False) | {'snake_length': 15, 'prev_action': 2} | [[[ 0 26  1  6  7]\n",
      "  [ 0  0  2  5  8]\n",
      "  [ 0  0  3  4  9]\n",
      "  [ 0 15 14 13 10]\n",
      "  [26  0  0 12 11]]]\n",
      "step 0/100, score: 13 (1 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 2} | [[[26  1  2  7  8]\n",
      "  [ 0  0  3  6  9]\n",
      "  [ 0  0  4  5 10]\n",
      "  [ 0 16 15 14 11]\n",
      "  [26  0  0 13 12]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 3} | [[[26  2  3  8  9]\n",
      "  [ 0  1  4  7 10]\n",
      "  [ 0  0  5  6 11]\n",
      "  [ 0  0 16 15 12]\n",
      "  [26  0  0 14 13]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 0} | [[[26  3  4  9 10]\n",
      "  [ 1  2  5  8 11]\n",
      "  [ 0  0  6  7 12]\n",
      "  [ 0  0  0 16 13]\n",
      "  [26  0  0 15 14]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 3} | [[[26  4  5 10 11]\n",
      "  [ 2  3  6  9 12]\n",
      "  [ 1  0  7  8 13]\n",
      "  [ 0  0  0  0 14]\n",
      "  [26  0  0 16 15]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 0} | [[[26  5  6 11 12]\n",
      "  [ 3  4  7 10 13]\n",
      "  [ 2  0  8  9 14]\n",
      "  [ 1  0  0  0 15]\n",
      "  [26  0  0  0 16]]]\n",
      "step 0/100, score: 14 (1 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 0} | [[[26  6  7 12 13]\n",
      "  [ 4  5  8 11 14]\n",
      "  [ 3  0  9 10 15]\n",
      "  [ 2 26  0  0 16]\n",
      "  [ 1  0  0  0 17]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 0} | [[[26  7  8 13 14]\n",
      "  [ 5  6  9 12 15]\n",
      "  [ 4  0 10 11 16]\n",
      "  [ 3 26  0  0 17]\n",
      "  [ 2  1  0  0  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26  8  9 14 15]\n",
      "  [ 6  7 10 13 16]\n",
      "  [ 5  0 11 12 17]\n",
      "  [ 4 26  0  0  0]\n",
      "  [ 3  2  1  0  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26  9 10 15 16]\n",
      "  [ 7  8 11 14 17]\n",
      "  [ 6  0 12 13  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 4  3  2  1  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26 10 11 16 17]\n",
      "  [ 8  9 12 15  0]\n",
      "  [ 7  0 13 14  0]\n",
      "  [ 6 26  0  0  0]\n",
      "  [ 5  4  3  2  1]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26 11 12 17  0]\n",
      "  [ 9 10 13 16  0]\n",
      "  [ 8  0 14 15  0]\n",
      "  [ 7 26  0  0  1]\n",
      "  [ 6  5  4  3  2]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 2} | [[[26 12 13  0  0]\n",
      "  [10 11 14 17  0]\n",
      "  [ 9  0 15 16  0]\n",
      "  [ 8 26  0  1  2]\n",
      "  [ 7  6  5  4  3]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 3} | [[[26 13 14  0  0]\n",
      "  [11 12 15  0  0]\n",
      "  [10  0 16 17  0]\n",
      "  [ 9 26  1  2  3]\n",
      "  [ 8  7  6  5  4]]]\n",
      "step 0/100, score: 15 (1 | trunc:False | trmt:False) | {'snake_length': 18, 'prev_action': 3} | [[[26 14 15 26  0]\n",
      "  [12 13 16  0  0]\n",
      "  [11  0 17 18  0]\n",
      "  [10  1  2  3  4]\n",
      "  [ 9  8  7  6  5]]]\n",
      "step 0/100, score: 15 (0 | trunc:False | trmt:False) | {'snake_length': 18, 'prev_action': 3} | [[[26 15 16 26  0]\n",
      "  [13 14 17  0  0]\n",
      "  [12  1 18  0  0]\n",
      "  [11  2  3  4  5]\n",
      "  [10  9  8  7  6]]]\n",
      "step 0/100, score: 14 (-1 | trunc:False | trmt:True) | {'snake_length': 18, 'prev_action': 2} | [[[26 15 16 26  0]\n",
      "  [13 14 17  0  0]\n",
      "  [12  1 18  0  0]\n",
      "  [11  2  3  4  5]\n",
      "  [10  9  8  7  6]]]\n",
      "GameOver!  total score: 14\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31merror\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     28\u001B[39m action = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m PLAY_MANUAL:\n\u001B[32m     31\u001B[39m     \u001B[38;5;66;03m# wait for key press\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpygame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     33\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m event.type == pygame.QUIT:\n\u001B[32m     34\u001B[39m             env.close()\n",
      "\u001B[31merror\u001B[39m: video system not initialized"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Approach 1",
   "id": "aa119f5229e6f30b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:55.725685363Z",
     "start_time": "2026-01-01T07:32:55.703243957Z"
    }
   },
   "source": [
    "class SnakeV0(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(SnakeV0, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:55.915593944Z",
     "start_time": "2026-01-01T07:32:55.895298613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "6be1b2b38add50d7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:56.126576846Z",
     "start_time": "2026-01-01T07:32:56.056508294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, modelclass, state_size, action_size, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "\n",
    "        self.q_net = modelclass(state_size, action_size).to(device)\n",
    "        self.target_net = modelclass(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())"
   ],
   "id": "2a2b483713b69b37",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## utils",
   "id": "1a0f859b6a869d85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:56.336456846Z",
     "start_time": "2026-01-01T07:32:56.286238777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_reward(\n",
    "    raw_reward,\n",
    "    terminated,\n",
    "    snake_length,\n",
    "    board_size,\n",
    "    steps_since_last_food,\n",
    "    max_steps_without_food\n",
    "):\n",
    "    \"\"\"\n",
    "    Unified reward function with shaped incentives.\n",
    "\n",
    "    Args:\n",
    "        raw_reward (float): Reward from environment (typically +1 for food).\n",
    "        terminated (bool): True if snake died.\n",
    "        snake_length (int): Current length of the snake.\n",
    "        board_size (int): Side length of square board.\n",
    "        steps_since_last_food (int): Number of steps since last food eaten.\n",
    "        max_steps_without_food (int): Threshold for looping penalty (set to board_size**2).\n",
    "\n",
    "    Returns:\n",
    "        float: Shaped reward for the current step.\n",
    "    \"\"\"\n",
    "    # Full board completion\n",
    "    if snake_length == board_size * board_size:\n",
    "        return 100.0\n",
    "\n",
    "    # Death penalty\n",
    "    if terminated:\n",
    "        return -10.0\n",
    "\n",
    "    reward = 0.0\n",
    "\n",
    "    # Food reward\n",
    "    if raw_reward > 0:\n",
    "        reward += 1.0\n",
    "\n",
    "    # Small step penalty to encourage efficiency\n",
    "    reward -= 0.01\n",
    "\n",
    "    # Looping penalty: no food in too long\n",
    "    if steps_since_last_food >= max_steps_without_food:\n",
    "        reward -= 0.1\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def train_dqn(\n",
    "    modelclass,\n",
    "    board_size=10,\n",
    "    env_name=\"gym_snakegame/SnakeGame-v0\",\n",
    "    episodes=20000,\n",
    "    max_steps=500\n",
    "):\n",
    "    env = gym.make(\n",
    "        env_name,\n",
    "        board_size=board_size,\n",
    "        n_channel=1,\n",
    "        n_target=1,\n",
    "        render_mode=None\n",
    "    )\n",
    "    state_size = np.prod(env.observation_space.shape)\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(modelclass=modelclass, state_size=state_size, action_size=action_size)\n",
    "\n",
    "    scores = deque(maxlen=100)\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "    epsilon_end = 0.01\n",
    "    max_steps_without_food = board_size * board_size\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = obs.flatten().astype(np.float32)\n",
    "        total_reward = 0.0\n",
    "        steps_since_last_food = 0\n",
    "        initial_length = info.get('snake_length', 1)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_obs, raw_reward, terminated, truncated, next_info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            current_length = next_info.get('snake_length', initial_length)\n",
    "            ate_food = (raw_reward > 0) or (current_length > initial_length + steps_since_last_food)\n",
    "\n",
    "            if ate_food:\n",
    "                steps_since_last_food = 0\n",
    "            else:\n",
    "                steps_since_last_food += 1\n",
    "\n",
    "            reward = compute_reward(\n",
    "                raw_reward=raw_reward,\n",
    "                terminated=terminated,\n",
    "                snake_length=current_length,\n",
    "                board_size=board_size,\n",
    "                steps_since_last_food=steps_since_last_food,\n",
    "                max_steps_without_food=max_steps_without_food\n",
    "            )\n",
    "\n",
    "            next_state = next_obs.flatten().astype(np.float32)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            initial_length = current_length\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores)\n",
    "            print(f\"Episode {episode}, Avg Reward (last 100): {avg_score:.3f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, board_size=10, episodes=100, max_steps=500):\n",
    "    env = gym.make(\n",
    "        \"gym_snakegame/SnakeGame-v0\",\n",
    "        board_size=board_size,\n",
    "        n_channel=1,\n",
    "        n_target=1,\n",
    "        render_mode=\"human\"\n",
    "    )\n",
    "    max_steps_without_food = board_size * board_size\n",
    "    total_reward = 0.0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = obs.flatten().astype(np.float32)\n",
    "        steps_since_last_food = 0\n",
    "        initial_length = info.get('snake_length', 1)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)\n",
    "            next_obs, raw_reward, terminated, truncated, next_info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            current_length = next_info.get('snake_length', initial_length)\n",
    "            ate_food = (raw_reward > 0) or (current_length > initial_length + steps_since_last_food)\n",
    "\n",
    "            if ate_food:\n",
    "                steps_since_last_food = 0\n",
    "            else:\n",
    "                steps_since_last_food += 1\n",
    "\n",
    "            reward = compute_reward(\n",
    "                raw_reward=raw_reward,\n",
    "                terminated=terminated,\n",
    "                snake_length=current_length,\n",
    "                board_size=board_size,\n",
    "                steps_since_last_food=steps_since_last_food,\n",
    "                max_steps_without_food=max_steps_without_food\n",
    "            )\n",
    "\n",
    "            state = next_obs.flatten().astype(np.float32)\n",
    "            episode_reward += reward\n",
    "            initial_length = current_length\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    print(f\"Evaluation over {episodes} episodes: Average Reward = {avg_reward:.2f}\")\n",
    "    env.close()"
   ],
   "id": "e8fa3c3d66b2c002",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "10e7802d31f65a8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:36:09.394888301Z",
     "start_time": "2026-01-01T07:32:59.186482616Z"
    }
   },
   "cell_type": "code",
   "source": "trained_agent = train_dqn(modelclass=SnakeV0, board_size=10, episodes=20_000)",
   "id": "e0f610febe71bf59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 0, Avg Reward (last 100): -10.060, Epsilon: 0.995\n",
      "Episode 100, Avg Reward (last 100): -9.977, Epsilon: 0.603\n",
      "Episode 200, Avg Reward (last 100): -10.030, Epsilon: 0.365\n",
      "Episode 300, Avg Reward (last 100): -9.994, Epsilon: 0.221\n",
      "Episode 400, Avg Reward (last 100): -10.035, Epsilon: 0.134\n",
      "Episode 500, Avg Reward (last 100): -10.011, Epsilon: 0.081\n",
      "Episode 600, Avg Reward (last 100): -10.030, Epsilon: 0.049\n",
      "Episode 700, Avg Reward (last 100): -10.023, Epsilon: 0.030\n",
      "Episode 800, Avg Reward (last 100): -10.009, Epsilon: 0.018\n",
      "Episode 900, Avg Reward (last 100): -9.991, Epsilon: 0.011\n",
      "Episode 1000, Avg Reward (last 100): -9.995, Epsilon: 0.010\n",
      "Episode 1100, Avg Reward (last 100): -9.996, Epsilon: 0.010\n",
      "Episode 1200, Avg Reward (last 100): -9.883, Epsilon: 0.010\n",
      "Episode 1300, Avg Reward (last 100): -9.997, Epsilon: 0.010\n",
      "Episode 1400, Avg Reward (last 100): -9.956, Epsilon: 0.010\n",
      "Episode 1500, Avg Reward (last 100): -10.005, Epsilon: 0.010\n",
      "Episode 1600, Avg Reward (last 100): -10.016, Epsilon: 0.010\n",
      "Episode 1700, Avg Reward (last 100): -9.967, Epsilon: 0.010\n",
      "Episode 1800, Avg Reward (last 100): -9.985, Epsilon: 0.010\n",
      "Episode 1900, Avg Reward (last 100): -10.023, Epsilon: 0.010\n",
      "Episode 2000, Avg Reward (last 100): -9.975, Epsilon: 0.010\n",
      "Episode 2100, Avg Reward (last 100): -10.015, Epsilon: 0.010\n",
      "Episode 2200, Avg Reward (last 100): -9.895, Epsilon: 0.010\n",
      "Episode 2300, Avg Reward (last 100): -9.964, Epsilon: 0.010\n",
      "Episode 2400, Avg Reward (last 100): -9.954, Epsilon: 0.010\n",
      "Episode 2500, Avg Reward (last 100): -9.966, Epsilon: 0.010\n",
      "Episode 2600, Avg Reward (last 100): -9.945, Epsilon: 0.010\n",
      "Episode 2700, Avg Reward (last 100): -9.904, Epsilon: 0.010\n",
      "Episode 2800, Avg Reward (last 100): -9.993, Epsilon: 0.010\n",
      "Episode 2900, Avg Reward (last 100): -9.946, Epsilon: 0.010\n",
      "Episode 3000, Avg Reward (last 100): -9.930, Epsilon: 0.010\n",
      "Episode 3100, Avg Reward (last 100): -9.932, Epsilon: 0.010\n",
      "Episode 3200, Avg Reward (last 100): -10.013, Epsilon: 0.010\n",
      "Episode 3300, Avg Reward (last 100): -9.961, Epsilon: 0.010\n",
      "Episode 3400, Avg Reward (last 100): -9.941, Epsilon: 0.010\n",
      "Episode 3500, Avg Reward (last 100): -9.962, Epsilon: 0.010\n",
      "Episode 3600, Avg Reward (last 100): -9.941, Epsilon: 0.010\n",
      "Episode 3700, Avg Reward (last 100): -9.944, Epsilon: 0.010\n",
      "Episode 3800, Avg Reward (last 100): -9.930, Epsilon: 0.010\n",
      "Episode 3900, Avg Reward (last 100): -9.931, Epsilon: 0.010\n",
      "Episode 4000, Avg Reward (last 100): -9.951, Epsilon: 0.010\n",
      "Episode 4100, Avg Reward (last 100): -9.937, Epsilon: 0.010\n",
      "Episode 4200, Avg Reward (last 100): -9.881, Epsilon: 0.010\n",
      "Episode 4300, Avg Reward (last 100): -9.954, Epsilon: 0.010\n",
      "Episode 4400, Avg Reward (last 100): -9.925, Epsilon: 0.010\n",
      "Episode 4500, Avg Reward (last 100): -9.987, Epsilon: 0.010\n",
      "Episode 4600, Avg Reward (last 100): -9.968, Epsilon: 0.010\n",
      "Episode 4700, Avg Reward (last 100): -9.920, Epsilon: 0.010\n",
      "Episode 4800, Avg Reward (last 100): -9.931, Epsilon: 0.010\n",
      "Episode 4900, Avg Reward (last 100): -9.928, Epsilon: 0.010\n",
      "Episode 5000, Avg Reward (last 100): -9.948, Epsilon: 0.010\n",
      "Episode 5100, Avg Reward (last 100): -9.919, Epsilon: 0.010\n",
      "Episode 5200, Avg Reward (last 100): -9.956, Epsilon: 0.010\n",
      "Episode 5300, Avg Reward (last 100): -9.932, Epsilon: 0.010\n",
      "Episode 5400, Avg Reward (last 100): -9.881, Epsilon: 0.010\n",
      "Episode 5500, Avg Reward (last 100): -9.983, Epsilon: 0.010\n",
      "Episode 5600, Avg Reward (last 100): -9.925, Epsilon: 0.010\n",
      "Episode 5700, Avg Reward (last 100): -9.952, Epsilon: 0.010\n",
      "Episode 5800, Avg Reward (last 100): -9.942, Epsilon: 0.010\n",
      "Episode 5900, Avg Reward (last 100): -9.951, Epsilon: 0.010\n",
      "Episode 6000, Avg Reward (last 100): -9.915, Epsilon: 0.010\n",
      "Episode 6100, Avg Reward (last 100): -9.912, Epsilon: 0.010\n",
      "Episode 6200, Avg Reward (last 100): -9.950, Epsilon: 0.010\n",
      "Episode 6300, Avg Reward (last 100): -9.935, Epsilon: 0.010\n",
      "Episode 6400, Avg Reward (last 100): -9.935, Epsilon: 0.010\n",
      "Episode 6500, Avg Reward (last 100): -9.925, Epsilon: 0.010\n",
      "Episode 6600, Avg Reward (last 100): -9.936, Epsilon: 0.010\n",
      "Episode 6700, Avg Reward (last 100): -9.946, Epsilon: 0.010\n",
      "Episode 6800, Avg Reward (last 100): -9.951, Epsilon: 0.010\n",
      "Episode 6900, Avg Reward (last 100): -9.971, Epsilon: 0.010\n",
      "Episode 7000, Avg Reward (last 100): -9.992, Epsilon: 0.010\n",
      "Episode 7100, Avg Reward (last 100): -9.927, Epsilon: 0.010\n",
      "Episode 7200, Avg Reward (last 100): -10.003, Epsilon: 0.010\n",
      "Episode 7300, Avg Reward (last 100): -9.963, Epsilon: 0.010\n",
      "Episode 7400, Avg Reward (last 100): -9.975, Epsilon: 0.010\n",
      "Episode 7500, Avg Reward (last 100): -9.935, Epsilon: 0.010\n",
      "Episode 7600, Avg Reward (last 100): -10.007, Epsilon: 0.010\n",
      "Episode 7700, Avg Reward (last 100): -9.918, Epsilon: 0.010\n",
      "Episode 7800, Avg Reward (last 100): -9.980, Epsilon: 0.010\n",
      "Episode 7900, Avg Reward (last 100): -9.989, Epsilon: 0.010\n",
      "Episode 8000, Avg Reward (last 100): -10.021, Epsilon: 0.010\n",
      "Episode 8100, Avg Reward (last 100): -10.001, Epsilon: 0.010\n",
      "Episode 8200, Avg Reward (last 100): -9.986, Epsilon: 0.010\n",
      "Episode 8300, Avg Reward (last 100): -9.928, Epsilon: 0.010\n",
      "Episode 8400, Avg Reward (last 100): -9.977, Epsilon: 0.010\n",
      "Episode 8500, Avg Reward (last 100): -9.952, Epsilon: 0.010\n",
      "Episode 8600, Avg Reward (last 100): -9.934, Epsilon: 0.010\n",
      "Episode 8700, Avg Reward (last 100): -10.010, Epsilon: 0.010\n",
      "Episode 8800, Avg Reward (last 100): -9.982, Epsilon: 0.010\n",
      "Episode 8900, Avg Reward (last 100): -9.872, Epsilon: 0.010\n",
      "Episode 9000, Avg Reward (last 100): -9.955, Epsilon: 0.010\n",
      "Episode 9100, Avg Reward (last 100): -9.957, Epsilon: 0.010\n",
      "Episode 9200, Avg Reward (last 100): -9.865, Epsilon: 0.010\n",
      "Episode 9300, Avg Reward (last 100): -9.953, Epsilon: 0.010\n",
      "Episode 9400, Avg Reward (last 100): -9.890, Epsilon: 0.010\n",
      "Episode 9500, Avg Reward (last 100): -9.922, Epsilon: 0.010\n",
      "Episode 9600, Avg Reward (last 100): -9.964, Epsilon: 0.010\n",
      "Episode 9700, Avg Reward (last 100): -9.921, Epsilon: 0.010\n",
      "Episode 9800, Avg Reward (last 100): -9.973, Epsilon: 0.010\n",
      "Episode 9900, Avg Reward (last 100): -9.959, Epsilon: 0.010\n",
      "Episode 10000, Avg Reward (last 100): -9.944, Epsilon: 0.010\n",
      "Episode 10100, Avg Reward (last 100): -9.944, Epsilon: 0.010\n",
      "Episode 10200, Avg Reward (last 100): -9.998, Epsilon: 0.010\n",
      "Episode 10300, Avg Reward (last 100): -10.015, Epsilon: 0.010\n",
      "Episode 10400, Avg Reward (last 100): -9.972, Epsilon: 0.010\n",
      "Episode 10500, Avg Reward (last 100): -10.015, Epsilon: 0.010\n",
      "Episode 10600, Avg Reward (last 100): -9.971, Epsilon: 0.010\n",
      "Episode 10700, Avg Reward (last 100): -9.922, Epsilon: 0.010\n",
      "Episode 10800, Avg Reward (last 100): -9.984, Epsilon: 0.010\n",
      "Episode 10900, Avg Reward (last 100): -10.033, Epsilon: 0.010\n",
      "Episode 11000, Avg Reward (last 100): -10.024, Epsilon: 0.010\n",
      "Episode 11100, Avg Reward (last 100): -9.969, Epsilon: 0.010\n",
      "Episode 11200, Avg Reward (last 100): -9.959, Epsilon: 0.010\n",
      "Episode 11300, Avg Reward (last 100): -9.937, Epsilon: 0.010\n",
      "Episode 11400, Avg Reward (last 100): -9.993, Epsilon: 0.010\n",
      "Episode 11500, Avg Reward (last 100): -9.950, Epsilon: 0.010\n",
      "Episode 11600, Avg Reward (last 100): -9.938, Epsilon: 0.010\n",
      "Episode 11700, Avg Reward (last 100): -9.889, Epsilon: 0.010\n",
      "Episode 11800, Avg Reward (last 100): -9.944, Epsilon: 0.010\n",
      "Episode 11900, Avg Reward (last 100): -9.988, Epsilon: 0.010\n",
      "Episode 12000, Avg Reward (last 100): -9.930, Epsilon: 0.010\n",
      "Episode 12100, Avg Reward (last 100): -10.007, Epsilon: 0.010\n",
      "Episode 12200, Avg Reward (last 100): -9.942, Epsilon: 0.010\n",
      "Episode 12300, Avg Reward (last 100): -9.887, Epsilon: 0.010\n",
      "Episode 12400, Avg Reward (last 100): -9.954, Epsilon: 0.010\n",
      "Episode 12500, Avg Reward (last 100): -10.006, Epsilon: 0.010\n",
      "Episode 12600, Avg Reward (last 100): -9.979, Epsilon: 0.010\n",
      "Episode 12700, Avg Reward (last 100): -9.985, Epsilon: 0.010\n",
      "Episode 12800, Avg Reward (last 100): -9.984, Epsilon: 0.010\n",
      "Episode 12900, Avg Reward (last 100): -9.997, Epsilon: 0.010\n",
      "Episode 13000, Avg Reward (last 100): -9.962, Epsilon: 0.010\n",
      "Episode 13100, Avg Reward (last 100): -10.015, Epsilon: 0.010\n",
      "Episode 13200, Avg Reward (last 100): -10.002, Epsilon: 0.010\n",
      "Episode 13300, Avg Reward (last 100): -9.979, Epsilon: 0.010\n",
      "Episode 13400, Avg Reward (last 100): -10.006, Epsilon: 0.010\n",
      "Episode 13500, Avg Reward (last 100): -9.967, Epsilon: 0.010\n",
      "Episode 13600, Avg Reward (last 100): -9.978, Epsilon: 0.010\n",
      "Episode 13700, Avg Reward (last 100): -9.964, Epsilon: 0.010\n",
      "Episode 13800, Avg Reward (last 100): -9.971, Epsilon: 0.010\n",
      "Episode 13900, Avg Reward (last 100): -10.001, Epsilon: 0.010\n",
      "Episode 14000, Avg Reward (last 100): -9.981, Epsilon: 0.010\n",
      "Episode 14100, Avg Reward (last 100): -10.001, Epsilon: 0.010\n",
      "Episode 14200, Avg Reward (last 100): -9.933, Epsilon: 0.010\n",
      "Episode 14300, Avg Reward (last 100): -9.972, Epsilon: 0.010\n",
      "Episode 14400, Avg Reward (last 100): -9.948, Epsilon: 0.010\n",
      "Episode 14500, Avg Reward (last 100): -9.971, Epsilon: 0.010\n",
      "Episode 14600, Avg Reward (last 100): -9.988, Epsilon: 0.010\n",
      "Episode 14700, Avg Reward (last 100): -9.980, Epsilon: 0.010\n",
      "Episode 14800, Avg Reward (last 100): -9.963, Epsilon: 0.010\n",
      "Episode 14900, Avg Reward (last 100): -9.951, Epsilon: 0.010\n",
      "Episode 15000, Avg Reward (last 100): -9.947, Epsilon: 0.010\n",
      "Episode 15100, Avg Reward (last 100): -9.977, Epsilon: 0.010\n",
      "Episode 15200, Avg Reward (last 100): -9.937, Epsilon: 0.010\n",
      "Episode 15300, Avg Reward (last 100): -10.004, Epsilon: 0.010\n",
      "Episode 15400, Avg Reward (last 100): -9.987, Epsilon: 0.010\n",
      "Episode 15500, Avg Reward (last 100): -9.960, Epsilon: 0.010\n",
      "Episode 15600, Avg Reward (last 100): -9.932, Epsilon: 0.010\n",
      "Episode 15700, Avg Reward (last 100): -9.987, Epsilon: 0.010\n",
      "Episode 15800, Avg Reward (last 100): -9.977, Epsilon: 0.010\n",
      "Episode 15900, Avg Reward (last 100): -9.898, Epsilon: 0.010\n",
      "Episode 16000, Avg Reward (last 100): -9.963, Epsilon: 0.010\n",
      "Episode 16100, Avg Reward (last 100): -9.993, Epsilon: 0.010\n",
      "Episode 16200, Avg Reward (last 100): -9.972, Epsilon: 0.010\n",
      "Episode 16300, Avg Reward (last 100): -9.991, Epsilon: 0.010\n",
      "Episode 16400, Avg Reward (last 100): -10.002, Epsilon: 0.010\n",
      "Episode 16500, Avg Reward (last 100): -10.002, Epsilon: 0.010\n",
      "Episode 16600, Avg Reward (last 100): -9.927, Epsilon: 0.010\n",
      "Episode 16700, Avg Reward (last 100): -9.924, Epsilon: 0.010\n",
      "Episode 16800, Avg Reward (last 100): -9.973, Epsilon: 0.010\n",
      "Episode 16900, Avg Reward (last 100): -9.983, Epsilon: 0.010\n",
      "Episode 17000, Avg Reward (last 100): -9.937, Epsilon: 0.010\n",
      "Episode 17100, Avg Reward (last 100): -9.953, Epsilon: 0.010\n",
      "Episode 17200, Avg Reward (last 100): -9.957, Epsilon: 0.010\n",
      "Episode 17300, Avg Reward (last 100): -9.977, Epsilon: 0.010\n",
      "Episode 17400, Avg Reward (last 100): -9.952, Epsilon: 0.010\n",
      "Episode 17500, Avg Reward (last 100): -9.954, Epsilon: 0.010\n",
      "Episode 17600, Avg Reward (last 100): -10.004, Epsilon: 0.010\n",
      "Episode 17700, Avg Reward (last 100): -9.985, Epsilon: 0.010\n",
      "Episode 17800, Avg Reward (last 100): -9.936, Epsilon: 0.010\n",
      "Episode 17900, Avg Reward (last 100): -9.948, Epsilon: 0.010\n",
      "Episode 18000, Avg Reward (last 100): -9.947, Epsilon: 0.010\n",
      "Episode 18100, Avg Reward (last 100): -10.026, Epsilon: 0.010\n",
      "Episode 18200, Avg Reward (last 100): -9.949, Epsilon: 0.010\n",
      "Episode 18300, Avg Reward (last 100): -10.021, Epsilon: 0.010\n",
      "Episode 18400, Avg Reward (last 100): -9.970, Epsilon: 0.010\n",
      "Episode 18500, Avg Reward (last 100): -9.980, Epsilon: 0.010\n",
      "Episode 18600, Avg Reward (last 100): -10.001, Epsilon: 0.010\n",
      "Episode 18700, Avg Reward (last 100): -9.985, Epsilon: 0.010\n",
      "Episode 18800, Avg Reward (last 100): -9.978, Epsilon: 0.010\n",
      "Episode 18900, Avg Reward (last 100): -10.016, Epsilon: 0.010\n",
      "Episode 19000, Avg Reward (last 100): -9.954, Epsilon: 0.010\n",
      "Episode 19100, Avg Reward (last 100): -9.937, Epsilon: 0.010\n",
      "Episode 19200, Avg Reward (last 100): -9.988, Epsilon: 0.010\n",
      "Episode 19300, Avg Reward (last 100): -10.005, Epsilon: 0.010\n",
      "Episode 19400, Avg Reward (last 100): -10.037, Epsilon: 0.010\n",
      "Episode 19500, Avg Reward (last 100): -9.974, Epsilon: 0.010\n",
      "Episode 19600, Avg Reward (last 100): -9.977, Epsilon: 0.010\n",
      "Episode 19700, Avg Reward (last 100): -9.942, Epsilon: 0.010\n",
      "Episode 19800, Avg Reward (last 100): -9.963, Epsilon: 0.010\n",
      "Episode 19900, Avg Reward (last 100): -10.017, Epsilon: 0.010\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:15.846544633Z",
     "start_time": "2026-01-01T07:32:15.800057291Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a2bdd4917074a878",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T04:40:12.572135665Z",
     "start_time": "2025-12-26T04:38:51.102318720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate_agent(trained_agent, board_size=10, episodes=10)\n",
    "eval_env = gym.make(\"gym_snakegame/SnakeGame-v0\",\n",
    "                    board_size=10,\n",
    "                    n_channel=1,\n",
    "                    n_target=1,\n",
    "                    render_mode=\"human\")\n",
    "evaluate_agent(agent, eval_env, episodes=200)\n",
    "eval_env.close()"
   ],
   "id": "a378bbc63b8e2b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success rate over 200 episodes: -200/200 (-100.0%)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2d114e1aa24b0d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
