{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Snake 10x10",
   "id": "3c29c28ed1582b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:11:48.424910305Z",
     "start_time": "2026-01-01T08:11:47.533166892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import gym_snakegame\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"PyTorch device:\", device)"
   ],
   "id": "89bb63c31604d604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the gymnasium env",
   "id": "50f2a5f052929594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:29:59.641169603Z",
     "start_time": "2026-01-01T07:29:41.654526595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PLAY_MANUAL = True  # set to False if u wanna see random moves\n",
    "env = gym.make(\n",
    "    \"gym_snakegame/SnakeGame-v0\",\n",
    "    board_size=5,\n",
    "    n_channel=1,\n",
    "    n_target=1,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "KEY_MAP = {\n",
    "    pygame.K_UP: 2,\n",
    "    pygame.K_RIGHT: 1,\n",
    "    pygame.K_DOWN: 0,\n",
    "    pygame.K_LEFT: 3\n",
    "}\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"hit arrows to play! or close window to quit\")\n",
    "if not PLAY_MANUAL:\n",
    "    print(\"auto mode: doing 250 random steps...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        action = None\n",
    "\n",
    "        if PLAY_MANUAL:\n",
    "            # wait for key press\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    env.close()\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key in KEY_MAP:\n",
    "                        action = KEY_MAP[event.key]\n",
    "            if action is None:\n",
    "                continue  # wait till key pressed\n",
    "        else:\n",
    "            if steps >= 250:\n",
    "                break\n",
    "            action = env.action_space.sample()\n",
    "            steps += 1\n",
    "\n",
    "        # step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        print(\n",
    "            f\"step {steps}/100, score: {total_reward} ({reward} | trunc:{truncated} | trmt:{terminated}) | {info} | {obs}\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"GameOver!  total score: {total_reward}\")\n",
    "            # reset everything\n",
    "            obs, info = env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ],
   "id": "99932bac4b3f83ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit arrows to play! or close window to quit\n",
      "step 0/100, score: 0 (0 | trunc:False | trmt:False) | {'snake_length': 3, 'prev_action': 1} | [[[ 0  0  0  0  0]\n",
      "  [26  0  1  0  0]\n",
      "  [ 0  3  2  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 0 (0 | trunc:False | trmt:False) | {'snake_length': 3, 'prev_action': 2} | [[[ 0  0  0  0  0]\n",
      "  [26  1  2  0  0]\n",
      "  [ 0  0  3  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (1 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 1  2  3  0  0]\n",
      "  [ 0  0  4  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 2  3  4  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 3  4  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 0 26  0  0  0]]]\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False) | {'snake_length': 4, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 4  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 1 26  0  0  0]]]\n",
      "step 0/100, score: 2 (1 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 0} | [[[ 0 26  0  0  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 4  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 2  1  0  0  0]]]\n",
      "step 0/100, score: 2 (0 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 1} | [[[ 0 26  0  0  0]\n",
      "  [ 0 26  0  0  0]\n",
      "  [ 5  0  0  0  0]\n",
      "  [ 4  1  0  0  0]\n",
      "  [ 3  2  0  0  0]]]\n",
      "step 0/100, score: 2 (0 | trunc:False | trmt:False) | {'snake_length': 5, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0 26  0  0  0]\n",
      "  [ 0  1  0  0  0]\n",
      "  [ 5  2  0  0  0]\n",
      "  [ 4  3  0  0  0]]]\n",
      "step 0/100, score: 3 (1 | trunc:False | trmt:False) | {'snake_length': 6, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0  1  0  0  0]\n",
      "  [ 0  2 26  0  0]\n",
      "  [ 6  3  0  0  0]\n",
      "  [ 5  4  0  0  0]]]\n",
      "step 0/100, score: 4 (1 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 2} | [[[ 0  1  0  0  0]\n",
      "  [ 0  2  0  0  0]\n",
      "  [26  3 26  0  0]\n",
      "  [ 7  4  0  0  0]\n",
      "  [ 6  5  0  0  0]]]\n",
      "step 0/100, score: 4 (0 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 2} | [[[ 0  2  1  0  0]\n",
      "  [ 0  3  0  0  0]\n",
      "  [26  4 26  0  0]\n",
      "  [ 0  5  0  0  0]\n",
      "  [ 7  6  0  0  0]]]\n",
      "step 0/100, score: 4 (0 | trunc:False | trmt:False) | {'snake_length': 7, 'prev_action': 1} | [[[ 0  3  2  0  0]\n",
      "  [ 0  4  1  0  0]\n",
      "  [26  5 26  0  0]\n",
      "  [ 0  6  0  0  0]\n",
      "  [ 0  7  0  0  0]]]\n",
      "step 0/100, score: 5 (1 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  4  3  0 26]\n",
      "  [ 0  5  2  0  0]\n",
      "  [26  6  1  0  0]\n",
      "  [ 0  7  0  0  0]\n",
      "  [ 0  8  0  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  5  4  0 26]\n",
      "  [ 0  6  3  0  0]\n",
      "  [26  7  2  0  0]\n",
      "  [ 0  8  1  0  0]\n",
      "  [ 0  0  0  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  6  5  0 26]\n",
      "  [ 0  7  4  0  0]\n",
      "  [26  8  3  0  0]\n",
      "  [ 0  0  2  0  0]\n",
      "  [ 0  0  1  0  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 0} | [[[ 0  7  6  0 26]\n",
      "  [ 0  8  5  0  0]\n",
      "  [26  0  4  0  0]\n",
      "  [ 0  0  3  0  0]\n",
      "  [ 0  0  2  1  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 1} | [[[ 0  8  7  0 26]\n",
      "  [ 0  0  6  0  0]\n",
      "  [26  0  5  0  0]\n",
      "  [ 0  0  4  1  0]\n",
      "  [ 0  0  3  2  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  8  0 26]\n",
      "  [ 0  0  7  0  0]\n",
      "  [26  0  6  1  0]\n",
      "  [ 0  0  5  2  0]\n",
      "  [ 0  0  4  3  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  0  0 26]\n",
      "  [ 0  0  8  1  0]\n",
      "  [26  0  7  2  0]\n",
      "  [ 0  0  6  3  0]\n",
      "  [ 0  0  5  4  0]]]\n",
      "step 0/100, score: 5 (0 | trunc:False | trmt:False) | {'snake_length': 8, 'prev_action': 2} | [[[ 0  0  0  1 26]\n",
      "  [ 0  0  0  2  0]\n",
      "  [26  0  8  3  0]\n",
      "  [ 0  0  7  4  0]\n",
      "  [ 0  0  6  5  0]]]\n",
      "step 0/100, score: 6 (1 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 2} | [[[ 0 26  0  2  1]\n",
      "  [ 0  0  0  3  0]\n",
      "  [26  0  9  4  0]\n",
      "  [ 0  0  8  5  0]\n",
      "  [ 0  0  7  6  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 1} | [[[ 0 26  0  3  2]\n",
      "  [ 0  0  0  4  1]\n",
      "  [26  0  0  5  0]\n",
      "  [ 0  0  9  6  0]\n",
      "  [ 0  0  8  7  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  4  3]\n",
      "  [ 0  0  0  5  2]\n",
      "  [26  0  0  6  1]\n",
      "  [ 0  0  0  7  0]\n",
      "  [ 0  0  9  8  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  5  4]\n",
      "  [ 0  0  0  6  3]\n",
      "  [26  0  0  7  2]\n",
      "  [ 0  0  0  8  1]\n",
      "  [ 0  0  0  9  0]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  6  5]\n",
      "  [ 0  0  0  7  4]\n",
      "  [26  0  0  8  3]\n",
      "  [ 0  0  0  9  2]\n",
      "  [ 0  0  0  0  1]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 0} | [[[ 0 26  0  7  6]\n",
      "  [ 0  0  0  8  5]\n",
      "  [26  0  0  9  4]\n",
      "  [ 0  0  0  0  3]\n",
      "  [ 0  0  0  1  2]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  8  7]\n",
      "  [ 0  0  0  9  6]\n",
      "  [26  0  0  0  5]\n",
      "  [ 0  0  0  0  4]\n",
      "  [ 0  0  1  2  3]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  9  8]\n",
      "  [ 0  0  0  0  7]\n",
      "  [26  0  0  0  6]\n",
      "  [ 0  0  0  0  5]\n",
      "  [ 0  1  2  3  4]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  0  9]\n",
      "  [ 0  0  0  0  8]\n",
      "  [26  0  0  0  7]\n",
      "  [ 0  0  0  0  6]\n",
      "  [ 1  2  3  4  5]]]\n",
      "step 0/100, score: 6 (0 | trunc:False | trmt:False) | {'snake_length': 9, 'prev_action': 3} | [[[ 0 26  0  0  0]\n",
      "  [ 0  0  0  0  9]\n",
      "  [26  0  0  0  8]\n",
      "  [ 1  0  0  0  7]\n",
      "  [ 2  3  4  5  6]]]\n",
      "step 0/100, score: 7 (1 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 0  0  0  0 10]\n",
      "  [ 1  0  0  0  9]\n",
      "  [ 2  0  0 26  8]\n",
      "  [ 3  4  5  6  7]]]\n",
      "step 0/100, score: 7 (0 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 0 26  0  0  0]\n",
      "  [ 1  0  0  0  0]\n",
      "  [ 2  0  0  0 10]\n",
      "  [ 3  0  0 26  9]\n",
      "  [ 4  5  6  7  8]]]\n",
      "step 0/100, score: 7 (0 | trunc:False | trmt:False) | {'snake_length': 10, 'prev_action': 2} | [[[ 1 26  0  0  0]\n",
      "  [ 2  0  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 4  0  0 26 10]\n",
      "  [ 5  6  7  8  9]]]\n",
      "step 0/100, score: 8 (1 | trunc:False | trmt:False) | {'snake_length': 11, 'prev_action': 2} | [[[ 2  1  0  0  0]\n",
      "  [ 3  0  0  0  0]\n",
      "  [ 4 26  0  0  0]\n",
      "  [ 5  0  0 26 11]\n",
      "  [ 6  7  8  9 10]]]\n",
      "step 0/100, score: 8 (0 | trunc:False | trmt:False) | {'snake_length': 11, 'prev_action': 1} | [[[ 3  2  0  0  0]\n",
      "  [ 4  1  0  0  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 6  0  0 26  0]\n",
      "  [ 7  8  9 10 11]]]\n",
      "step 0/100, score: 9 (1 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 4  3  0  0  0]\n",
      "  [ 5  2  0  0 26]\n",
      "  [ 6  1  0  0  0]\n",
      "  [ 7  0  0 26  0]\n",
      "  [ 8  9 10 11 12]]]\n",
      "step 0/100, score: 9 (0 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 5  4  0  0  0]\n",
      "  [ 6  3  0  0 26]\n",
      "  [ 7  2  0  0  0]\n",
      "  [ 8  1  0 26  0]\n",
      "  [ 9 10 11 12  0]]]\n",
      "step 0/100, score: 9 (0 | trunc:False | trmt:False) | {'snake_length': 12, 'prev_action': 0} | [[[ 6  5  0  0  0]\n",
      "  [ 7  4  0  0 26]\n",
      "  [ 8  3  0  0  0]\n",
      "  [ 9  2  1 26  0]\n",
      "  [10 11 12  0  0]]]\n",
      "step 0/100, score: 10 (1 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[ 7  6 26  0  0]\n",
      "  [ 8  5  0  0 26]\n",
      "  [ 9  4  0  0  0]\n",
      "  [10  3  2  1  0]\n",
      "  [11 12 13  0  0]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[ 8  7 26  0  0]\n",
      "  [ 9  6  0  0 26]\n",
      "  [10  5  0  0  0]\n",
      "  [11  4  3  2  0]\n",
      "  [12 13  0  1  0]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 0} | [[[ 9  8 26  0  0]\n",
      "  [10  7  0  0 26]\n",
      "  [11  6  0  0  0]\n",
      "  [12  5  4  3  0]\n",
      "  [13  0  0  2  1]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 1} | [[[10  9 26  0  0]\n",
      "  [11  8  0  0 26]\n",
      "  [12  7  0  0  0]\n",
      "  [13  6  5  4  1]\n",
      "  [ 0  0  0  3  2]]]\n",
      "step 0/100, score: 10 (0 | trunc:False | trmt:False) | {'snake_length': 13, 'prev_action': 2} | [[[11 10 26  0  0]\n",
      "  [12  9  0  0 26]\n",
      "  [13  8  0  0  1]\n",
      "  [ 0  7  6  5  2]\n",
      "  [ 0  0  0  4  3]]]\n",
      "step 0/100, score: 11 (1 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[12 11 26  0  0]\n",
      "  [13 10  0  0  1]\n",
      "  [14  9  0  0  2]\n",
      "  [ 0  8  7  6  3]\n",
      "  [26  0  0  5  4]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[13 12 26  0  1]\n",
      "  [14 11  0  0  2]\n",
      "  [ 0 10  0  0  3]\n",
      "  [ 0  9  8  7  4]\n",
      "  [26  0  0  6  5]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 2} | [[[14 13 26  1  2]\n",
      "  [ 0 12  0  0  3]\n",
      "  [ 0 11  0  0  4]\n",
      "  [ 0 10  9  8  5]\n",
      "  [26  0  0  7  6]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 3} | [[[ 0 14 26  2  3]\n",
      "  [ 0 13  0  1  4]\n",
      "  [ 0 12  0  0  5]\n",
      "  [ 0 11 10  9  6]\n",
      "  [26  0  0  8  7]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 0} | [[[ 0  0 26  3  4]\n",
      "  [ 0 14  0  2  5]\n",
      "  [ 0 13  0  1  6]\n",
      "  [ 0 12 11 10  7]\n",
      "  [26  0  0  9  8]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 0} | [[[ 0  0 26  4  5]\n",
      "  [ 0  0  0  3  6]\n",
      "  [ 0 14  1  2  7]\n",
      "  [ 0 13 12 11  8]\n",
      "  [26  0  0 10  9]]]\n",
      "step 0/100, score: 11 (0 | trunc:False | trmt:False) | {'snake_length': 14, 'prev_action': 3} | [[[ 0  0 26  5  6]\n",
      "  [ 0  0  1  4  7]\n",
      "  [ 0  0  2  3  8]\n",
      "  [ 0 14 13 12  9]\n",
      "  [26  0  0 11 10]]]\n",
      "step 0/100, score: 12 (1 | trunc:False | trmt:False) | {'snake_length': 15, 'prev_action': 2} | [[[ 0 26  1  6  7]\n",
      "  [ 0  0  2  5  8]\n",
      "  [ 0  0  3  4  9]\n",
      "  [ 0 15 14 13 10]\n",
      "  [26  0  0 12 11]]]\n",
      "step 0/100, score: 13 (1 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 2} | [[[26  1  2  7  8]\n",
      "  [ 0  0  3  6  9]\n",
      "  [ 0  0  4  5 10]\n",
      "  [ 0 16 15 14 11]\n",
      "  [26  0  0 13 12]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 3} | [[[26  2  3  8  9]\n",
      "  [ 0  1  4  7 10]\n",
      "  [ 0  0  5  6 11]\n",
      "  [ 0  0 16 15 12]\n",
      "  [26  0  0 14 13]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 0} | [[[26  3  4  9 10]\n",
      "  [ 1  2  5  8 11]\n",
      "  [ 0  0  6  7 12]\n",
      "  [ 0  0  0 16 13]\n",
      "  [26  0  0 15 14]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 3} | [[[26  4  5 10 11]\n",
      "  [ 2  3  6  9 12]\n",
      "  [ 1  0  7  8 13]\n",
      "  [ 0  0  0  0 14]\n",
      "  [26  0  0 16 15]]]\n",
      "step 0/100, score: 13 (0 | trunc:False | trmt:False) | {'snake_length': 16, 'prev_action': 0} | [[[26  5  6 11 12]\n",
      "  [ 3  4  7 10 13]\n",
      "  [ 2  0  8  9 14]\n",
      "  [ 1  0  0  0 15]\n",
      "  [26  0  0  0 16]]]\n",
      "step 0/100, score: 14 (1 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 0} | [[[26  6  7 12 13]\n",
      "  [ 4  5  8 11 14]\n",
      "  [ 3  0  9 10 15]\n",
      "  [ 2 26  0  0 16]\n",
      "  [ 1  0  0  0 17]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 0} | [[[26  7  8 13 14]\n",
      "  [ 5  6  9 12 15]\n",
      "  [ 4  0 10 11 16]\n",
      "  [ 3 26  0  0 17]\n",
      "  [ 2  1  0  0  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26  8  9 14 15]\n",
      "  [ 6  7 10 13 16]\n",
      "  [ 5  0 11 12 17]\n",
      "  [ 4 26  0  0  0]\n",
      "  [ 3  2  1  0  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26  9 10 15 16]\n",
      "  [ 7  8 11 14 17]\n",
      "  [ 6  0 12 13  0]\n",
      "  [ 5 26  0  0  0]\n",
      "  [ 4  3  2  1  0]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26 10 11 16 17]\n",
      "  [ 8  9 12 15  0]\n",
      "  [ 7  0 13 14  0]\n",
      "  [ 6 26  0  0  0]\n",
      "  [ 5  4  3  2  1]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 1} | [[[26 11 12 17  0]\n",
      "  [ 9 10 13 16  0]\n",
      "  [ 8  0 14 15  0]\n",
      "  [ 7 26  0  0  1]\n",
      "  [ 6  5  4  3  2]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 2} | [[[26 12 13  0  0]\n",
      "  [10 11 14 17  0]\n",
      "  [ 9  0 15 16  0]\n",
      "  [ 8 26  0  1  2]\n",
      "  [ 7  6  5  4  3]]]\n",
      "step 0/100, score: 14 (0 | trunc:False | trmt:False) | {'snake_length': 17, 'prev_action': 3} | [[[26 13 14  0  0]\n",
      "  [11 12 15  0  0]\n",
      "  [10  0 16 17  0]\n",
      "  [ 9 26  1  2  3]\n",
      "  [ 8  7  6  5  4]]]\n",
      "step 0/100, score: 15 (1 | trunc:False | trmt:False) | {'snake_length': 18, 'prev_action': 3} | [[[26 14 15 26  0]\n",
      "  [12 13 16  0  0]\n",
      "  [11  0 17 18  0]\n",
      "  [10  1  2  3  4]\n",
      "  [ 9  8  7  6  5]]]\n",
      "step 0/100, score: 15 (0 | trunc:False | trmt:False) | {'snake_length': 18, 'prev_action': 3} | [[[26 15 16 26  0]\n",
      "  [13 14 17  0  0]\n",
      "  [12  1 18  0  0]\n",
      "  [11  2  3  4  5]\n",
      "  [10  9  8  7  6]]]\n",
      "step 0/100, score: 14 (-1 | trunc:False | trmt:True) | {'snake_length': 18, 'prev_action': 2} | [[[26 15 16 26  0]\n",
      "  [13 14 17  0  0]\n",
      "  [12  1 18  0  0]\n",
      "  [11  2  3  4  5]\n",
      "  [10  9  8  7  6]]]\n",
      "GameOver!  total score: 14\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31merror\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     28\u001B[39m action = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m PLAY_MANUAL:\n\u001B[32m     31\u001B[39m     \u001B[38;5;66;03m# wait for key press\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpygame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     33\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m event.type == pygame.QUIT:\n\u001B[32m     34\u001B[39m             env.close()\n",
      "\u001B[31merror\u001B[39m: video system not initialized"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Approach 1",
   "id": "aa119f5229e6f30b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:11:51.439873281Z",
     "start_time": "2026-01-01T08:11:51.396170649Z"
    }
   },
   "source": [
    "class SnakeV0(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(SnakeV0, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:11:51.571521734Z",
     "start_time": "2026-01-01T08:11:51.553827295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "6be1b2b38add50d7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:11:51.736045040Z",
     "start_time": "2026-01-01T08:11:51.722628484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, modelclass, state_size, action_size, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=50000, batch_size=128, target_update=500, hidden_size=128):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "\n",
    "        self.q_net = modelclass(state_size, action_size, hidden_size).to(device)\n",
    "        self.target_net = modelclass(state_size, action_size, hidden_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ],
   "id": "2a2b483713b69b37",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## utils",
   "id": "1a0f859b6a869d85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:18:09.864802023Z",
     "start_time": "2026-01-01T08:18:09.800453945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_reward(\n",
    "    raw_reward,\n",
    "    terminated,\n",
    "    snake_length,\n",
    "    board_size,\n",
    "    steps_since_last_food,\n",
    "    current_step\n",
    "):\n",
    "    \"\"\"\n",
    "    Shaped reward to accelerate learning.\n",
    "    \"\"\"\n",
    "    max_len = board_size * board_size\n",
    "\n",
    "    # Win condition: filled board\n",
    "    if snake_length == max_len:\n",
    "        return 100.0\n",
    "\n",
    "    # Death\n",
    "    if terminated:\n",
    "        return -10.0\n",
    "\n",
    "    reward = 0.0\n",
    "\n",
    "    # Positive reward for eating\n",
    "    if raw_reward > 0:\n",
    "        reward += 1.0\n",
    "\n",
    "    # Small survival bonus to offset step cost and encourage exploration\n",
    "    reward += 0.1\n",
    "\n",
    "    # Very mild step cost (can be removed if needed)\n",
    "    reward -= 0.01\n",
    "\n",
    "    # Penalize extreme stagnation: no food for board_size^2 steps\n",
    "    if steps_since_last_food >= max_len and current_step > max_len:\n",
    "        reward -= 0.5\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def format_number_with_spaces(n):\n",
    "    \"\"\"Format large episode numbers with spaces as thousand separators.\"\"\"\n",
    "    return f\"{n:,}\".replace(\",\", \" \")\n",
    "\n",
    "def get_snake_length(obs, board_size):\n",
    "    \"\"\"Count number of snake cells in observation (n_channel=1).\"\"\"\n",
    "    # obs shape: (1, H, W) or (H, W) â€” handle both\n",
    "    grid = obs[0] if obs.ndim == 3 else obs\n",
    "    max_body_val = board_size * board_size\n",
    "    # Snake cells: values from 1 to max_body_val\n",
    "    return np.sum((grid >= 1) & (grid <= max_body_val))\n",
    "\n",
    "def train_dqn(\n",
    "    modelclass,\n",
    "    board_size=10,\n",
    "    env_name=\"gym_snakegame/SnakeGame-v0\",\n",
    "    episodes=20000,\n",
    "    max_steps=500,\n",
    "    plot_live=False\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    env = gym.make(\n",
    "        env_name,\n",
    "        board_size=board_size,\n",
    "        n_channel=1,\n",
    "        n_target=1,\n",
    "        render_mode=None\n",
    "    )\n",
    "    state_size = np.prod(env.observation_space.shape)\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(\n",
    "        modelclass=modelclass,\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        lr=1e-4,\n",
    "        buffer_size=50000,\n",
    "        batch_size=128,\n",
    "        target_update=500\n",
    "    )\n",
    "\n",
    "    # Tracking metrics\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    recent_steps = deque(maxlen=100)\n",
    "    recent_lengths = deque(maxlen=100)\n",
    "\n",
    "    # For plotting\n",
    "    episode_log = []\n",
    "    epsilon_log = []\n",
    "    reward_stats_log = []      # [min, q25, med, q75, max]\n",
    "    steps_stats_log = []\n",
    "    length_stats_log = []\n",
    "\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.9995\n",
    "    epsilon_end = 0.03\n",
    "    max_steps_without_food = board_size * board_size\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        state = obs.flatten().astype(np.float32)\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        steps_since_last_food = 0\n",
    "        prev_length = int(get_snake_length(obs, board_size))\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_obs, raw_reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            curr_length = int(get_snake_length(next_obs, board_size))\n",
    "            ate_food = (curr_length > prev_length)\n",
    "\n",
    "            if ate_food:\n",
    "                steps_since_last_food = 0\n",
    "            else:\n",
    "                steps_since_last_food += 1\n",
    "\n",
    "            reward = compute_reward(\n",
    "                raw_reward=raw_reward,\n",
    "                terminated=terminated,\n",
    "                snake_length=curr_length,\n",
    "                board_size=board_size,\n",
    "                steps_since_last_food=steps_since_last_food,\n",
    "                current_step=t\n",
    "            )\n",
    "\n",
    "            next_state = next_obs.flatten().astype(np.float32)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            prev_length = curr_length\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        final_length = curr_length\n",
    "        recent_rewards.append(total_reward)\n",
    "        recent_steps.append(steps)\n",
    "        recent_lengths.append(final_length)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        # Log every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            rewards = np.array(recent_rewards)\n",
    "            steps_arr = np.array(recent_steps)\n",
    "            lengths_arr = np.array(recent_lengths)\n",
    "\n",
    "            r_min, r_q25, r_med, r_q75, r_max = np.percentile(rewards, [0, 25, 50, 75, 100])\n",
    "            s_min, s_q25, s_med, s_q75, s_max = np.percentile(steps_arr, [0, 25, 50, 75, 100])\n",
    "            l_min, l_q25, l_med, l_q75, l_max = np.percentile(lengths_arr, [0, 25, 50, 75, 100])\n",
    "\n",
    "            ep_str = f\"{episode:,}\".replace(\",\", \" \")\n",
    "            print(\n",
    "                f\"Episode {ep_str:>12} | \"\n",
    "                f\"R (last 100): {r_min:+7.3f} / {r_q25:+7.3f} / {r_med:+7.3f} / {r_q75:+7.3f} / {r_max:+7.3f} | \"\n",
    "                f\"Steps: {s_min:5.0f} / {s_q25:5.0f} / {s_med:5.0f} / {s_q75:5.0f} / {s_max:5.0f} | \"\n",
    "                f\"Epsilon: {epsilon:6.3f}\"\n",
    "            )\n",
    "\n",
    "            # Store for plotting\n",
    "            episode_log.append(episode)\n",
    "            epsilon_log.append(epsilon)\n",
    "            reward_stats_log.append([r_min, r_q25, r_med, r_q75, r_max])\n",
    "            steps_stats_log.append([s_min, s_q25, s_med, s_q75, s_max])\n",
    "            length_stats_log.append([l_min, l_q25, l_med, l_q75, l_max])\n",
    "\n",
    "            # Real-time plot\n",
    "            if plot_live:\n",
    "                clear_output(wait=True)\n",
    "                fig, axs = plt.subplots(4, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "                episodes_plot = np.array(episode_log)\n",
    "\n",
    "                # Epsilon\n",
    "                axs[0].plot(episodes_plot, epsilon_log, 'b-', label='Epsilon')\n",
    "                axs[0].set_ylabel('Epsilon')\n",
    "                axs[0].grid(True)\n",
    "                axs[0].legend()\n",
    "\n",
    "                # Reward\n",
    "                reward_arr = np.array(reward_stats_log)\n",
    "                axs[1].fill_between(episodes_plot, reward_arr[:, 0], reward_arr[:, 4], color='lightcoral', alpha=0.3)\n",
    "                axs[1].fill_between(episodes_plot, reward_arr[:, 1], reward_arr[:, 3], color='lightcoral', alpha=0.5)\n",
    "                axs[1].plot(episodes_plot, reward_arr[:, 2], 'r-', label='Reward (median)')\n",
    "                axs[1].set_ylabel('Reward')\n",
    "                axs[1].grid(True)\n",
    "                axs[1].legend()\n",
    "\n",
    "                # Steps\n",
    "                steps_arr_plot = np.array(steps_stats_log)\n",
    "                axs[2].fill_between(episodes_plot, steps_arr_plot[:, 0], steps_arr_plot[:, 4], color='lightgreen', alpha=0.3)\n",
    "                axs[2].fill_between(episodes_plot, steps_arr_plot[:, 1], steps_arr_plot[:, 3], color='lightgreen', alpha=0.5)\n",
    "                axs[2].plot(episodes_plot, steps_arr_plot[:, 2], 'g-', label='Steps (median)')\n",
    "                axs[2].set_ylabel('Steps')\n",
    "                axs[2].grid(True)\n",
    "                axs[2].legend()\n",
    "\n",
    "                # Snake Length\n",
    "                length_arr = np.array(length_stats_log)\n",
    "                axs[3].fill_between(episodes_plot, length_arr[:, 0], length_arr[:, 4], color='lightblue', alpha=0.3)\n",
    "                axs[3].fill_between(episodes_plot, length_arr[:, 1], length_arr[:, 3], color='lightblue', alpha=0.5)\n",
    "                axs[3].plot(episodes_plot, length_arr[:, 2], 'b-', label='Length (median)')\n",
    "                axs[3].set_ylabel('Final Snake Length')\n",
    "                axs[3].set_xlabel('Episode')\n",
    "                axs[3].grid(True)\n",
    "                axs[3].legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, board_size=10, episodes=10, max_steps=500):\n",
    "    env = gym.make(\n",
    "        \"gym_snakegame/SnakeGame-v0\",\n",
    "        board_size=board_size,\n",
    "        n_channel=1,\n",
    "        n_target=1,\n",
    "        render_mode=\"human\"\n",
    "    )\n",
    "    total_reward = 0.0\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        state = obs.flatten().astype(np.float32)\n",
    "        prev_length = info.get('snake_length', 1)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)\n",
    "            obs, raw_reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = obs.flatten().astype(np.float32)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    env.close()"
   ],
   "id": "e8fa3c3d66b2c002",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "10e7802d31f65a8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T08:29:25.508920660Z",
     "start_time": "2026-01-01T08:18:11.157890188Z"
    }
   },
   "cell_type": "code",
   "source": "agent = train_dqn(modelclass=SnakeV0, board_size=5, episodes=20000, max_steps=2_000, plot_live=True)",
   "id": "e0f610febe71bf59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode           99 | R (last 100):  -9.820 /  -9.820 /  -9.640 /  -9.100 /  -6.740 | Steps:     3 /     3 /     4 /     7 /    26 | Epsilon:  0.951\n",
      "Episode          199 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -9.258 /  -6.830 | Steps:     3 /     4 /     5 /     7 /    16 | Epsilon:  0.905\n",
      "Episode          299 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -9.348 /  -7.190 | Steps:     3 /     4 /     4 /     6 /    19 | Epsilon:  0.861\n",
      "Episode          399 | R (last 100):  -9.820 /  -9.730 /  -9.595 /  -8.710 /  -6.010 | Steps:     3 /     4 /     5 /     7 /    19 | Epsilon:  0.819\n",
      "Episode          499 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -9.280 /  -7.190 | Steps:     3 /     4 /     5 /     6 /    15 | Epsilon:  0.779\n",
      "Episode          599 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -9.190 /  -6.650 | Steps:     3 /     4 /     5 /     6 /    16 | Epsilon:  0.741\n",
      "Episode          699 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -8.828 /  -6.740 | Steps:     3 /     4 /     5 /     6 /    16 | Epsilon:  0.705\n",
      "Episode          799 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -9.168 /  -6.460 | Steps:     3 /     4 /     5 /     6 /    17 | Epsilon:  0.670\n",
      "Episode          899 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -8.820 /  -6.640 | Steps:     3 /     4 /     5 /     7 /    15 | Epsilon:  0.638\n",
      "Episode          999 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -8.730 /  -5.290 | Steps:     3 /     4 /     5 /     9 /    27 | Epsilon:  0.606\n",
      "Episode        1 099 | R (last 100):  -9.820 /  -9.730 /  -9.460 /  -8.710 /  -6.740 | Steps:     3 /     4 /     5 /     7 /    24 | Epsilon:  0.577\n",
      "Episode        1 199 | R (last 100):  -9.820 /  -9.730 /  -9.550 /  -8.730 /  -6.380 | Steps:     3 /     4 /     5 /     7 /    19 | Epsilon:  0.549\n",
      "Episode        1 299 | R (last 100):  -9.820 /  -9.730 /  -9.640 /  -8.797 /  -4.390 | Steps:     3 /     4 /     5 /     6 /    30 | Epsilon:  0.522\n",
      "Episode        1 399 | R (last 100):  -9.820 /  -9.730 /  -9.550 /  -8.820 /  -6.740 | Steps:     3 /     4 /     5 /     7 /    18 | Epsilon:  0.496\n",
      "Episode        1 499 | R (last 100):  -9.820 /  -9.730 /  -9.280 /  -8.530 /  -5.740 | Steps:     3 /     4 /     5 /     9 /    19 | Epsilon:  0.472\n",
      "Episode        1 599 | R (last 100):  -9.820 /  -9.640 /  -9.370 /  -8.640 /  -6.470 | Steps:     3 /     4 /     5 /     8 /    18 | Epsilon:  0.449\n",
      "Episode        1 699 | R (last 100):  -9.820 /  -9.730 /  -9.460 /  -8.730 /  -7.100 | Steps:     3 /     4 /     5 /     8 /    21 | Epsilon:  0.427\n",
      "Episode        1 799 | R (last 100):  -9.820 /  -9.730 /  -9.460 /  -8.640 /  -6.830 | Steps:     3 /     4 /     5 /     8 /    25 | Epsilon:  0.406\n",
      "Episode        1 899 | R (last 100):  -9.820 /  -9.640 /  -9.190 /  -8.640 /  -7.010 | Steps:     3 /     4 /     5 /    10 /    26 | Epsilon:  0.387\n",
      "Episode        1 999 | R (last 100):  -9.820 /  -9.640 /  -9.190 /  -8.640 /  -6.740 | Steps:     3 /     4 /     5 /     9 /    25 | Epsilon:  0.368\n",
      "Episode        2 099 | R (last 100):  -9.820 /  -9.730 /  -9.190 /  -8.558 /  -6.640 | Steps:     3 /     4 /     5 /    11 /    22 | Epsilon:  0.350\n",
      "Episode        2 199 | R (last 100):  -9.820 /  -9.663 /  -8.780 /  -8.197 /  -5.840 | Steps:     3 /     4 /     5 /    10 /    30 | Epsilon:  0.333\n",
      "Episode        2 299 | R (last 100):  -9.820 /  -9.550 /  -8.820 /  -8.550 /  -4.840 | Steps:     3 /     4 /     6 /    11 /    34 | Epsilon:  0.317\n",
      "Episode        2 399 | R (last 100): -14.630 /  -9.640 /  -8.730 /  -7.987 /  -5.020 | Steps:     3 /     4 /     6 /    11 /    44 | Epsilon:  0.301\n",
      "Episode        2 499 | R (last 100): -14.630 /  -9.640 /  -8.825 /  -8.258 /  -5.120 | Steps:     3 /     4 /     8 /    12 /    47 | Epsilon:  0.286\n",
      "Episode        2 599 | R (last 100): -14.630 /  -9.640 /  -8.730 /  -8.260 /  -6.020 | Steps:     3 /     5 /     7 /    12 /    44 | Epsilon:  0.272\n",
      "Episode        2 699 | R (last 100): -10.120 /  -9.483 /  -8.730 /  -8.348 /  -4.920 | Steps:     3 /     5 /     7 /    12 /    33 | Epsilon:  0.259\n",
      "Episode        2 799 | R (last 100):  -9.820 /  -9.550 /  -8.730 /  -7.830 /  -5.920 | Steps:     3 /     5 /     8 /    13 /    32 | Epsilon:  0.247\n",
      "Episode        2 899 | R (last 100): -18.730 /  -9.190 /  -8.555 /  -7.707 /  -4.390 | Steps:     3 /     5 /     9 /    15 /    54 | Epsilon:  0.234\n",
      "Episode        2 999 | R (last 100): -10.530 /  -9.393 /  -8.640 /  -7.920 /  -4.120 | Steps:     3 /     5 /     9 /    13 /    34 | Epsilon:  0.223\n",
      "Episode        3 099 | R (last 100):  -9.820 /  -9.370 /  -8.640 /  -7.718 /  -5.020 | Steps:     3 /     5 /     9 /    16 /    32 | Epsilon:  0.212\n",
      "Episode        3 199 | R (last 100):  -9.820 /  -9.393 /  -8.730 /  -7.897 /  -3.210 | Steps:     3 /     5 /     8 /    15 /    35 | Epsilon:  0.202\n",
      "Episode        3 299 | R (last 100): -15.860 /  -9.122 /  -8.640 /  -7.640 /  +0.040 | Steps:     3 /     5 /    10 /    14 /    57 | Epsilon:  0.192\n",
      "Episode        3 399 | R (last 100): -12.170 /  -9.122 /  -8.510 /  -7.740 /  -4.470 | Steps:     3 /     6 /    12 /    19 /    41 | Epsilon:  0.183\n",
      "Episode        3 499 | R (last 100): -18.880 /  -9.483 /  -8.640 /  -7.080 /  -3.540 | Steps:     3 /     6 /    12 /    17 /    69 | Epsilon:  0.174\n",
      "Episode        3 599 | R (last 100): -38.410 /  -9.280 /  -8.420 /  -7.235 /  -3.020 | Steps:     3 /     6 /    12 /    19 /   102 | Epsilon:  0.165\n",
      "Episode        3 699 | R (last 100): -12.360 /  -8.640 /  -8.015 /  -7.288 /  -0.040 | Steps:     3 /     7 /    12 /    23 /    57 | Epsilon:  0.157\n",
      "Episode        3 799 | R (last 100): -21.190 /  -9.122 /  -8.190 /  -6.795 /  -2.940 | Steps:     3 /     6 /    14 /    24 /    63 | Epsilon:  0.149\n",
      "Episode        3 899 | R (last 100): -29.390 /  -9.460 /  -8.210 /  -7.355 /  -1.320 | Steps:     3 /     7 /    14 /    22 /    80 | Epsilon:  0.142\n",
      "Episode        3 999 | R (last 100): -18.730 /  -9.100 /  -7.920 /  -6.223 /  -2.940 | Steps:     3 /     8 /    16 /    27 /    65 | Epsilon:  0.135\n",
      "Episode        4 099 | R (last 100): -23.720 /  -9.302 /  -7.965 /  -6.028 /  -1.130 | Steps:     3 /     9 /    14 /    27 /    93 | Epsilon:  0.129\n",
      "Episode        4 199 | R (last 100): -12.580 /  -9.280 /  -7.750 /  -6.568 /  -0.600 | Steps:     3 /     9 /    14 /    27 /    61 | Epsilon:  0.122\n",
      "Episode        4 299 | R (last 100): -54.810 /  -9.460 /  -8.100 /  -5.885 /  -0.970 | Steps:     3 /     7 /    19 /    30 /   142 | Epsilon:  0.116\n",
      "Episode        4 399 | R (last 100): -19.960 /  -9.100 /  -8.010 /  -6.425 /  -2.040 | Steps:     3 /    10 /    15 /    23 /    83 | Epsilon:  0.111\n",
      "Episode        4 499 | R (last 100): -27.140 /  -9.663 /  -8.555 /  -6.355 /  -1.860 | Steps:     3 /     7 /    19 /    33 /   111 | Epsilon:  0.105\n",
      "Episode        4 599 | R (last 100): -45.300 /  -9.100 /  -8.190 /  -5.995 /  -1.140 | Steps:     3 /    12 /    20 /    33 /   131 | Epsilon:  0.100\n",
      "Episode        4 699 | R (last 100): -35.570 /  -9.393 /  -8.085 /  -6.170 /  -1.970 | Steps:     3 /    11 /    21 /    35 /   128 | Epsilon:  0.095\n",
      "Episode        4 799 | R (last 100): -52.220 /  -9.285 /  -7.425 /  -5.655 /  +2.490 | Steps:     3 /    14 /    23 /    40 /   143 | Epsilon:  0.091\n",
      "Episode        4 899 | R (last 100): -25.400 /  -9.445 /  -8.060 /  -6.158 /  -1.670 | Steps:     3 /    11 /    23 /    38 /   109 | Epsilon:  0.086\n",
      "Episode        4 999 | R (last 100): -40.870 /  -9.820 /  -8.065 /  -6.428 /  -1.220 | Steps:     3 /    15 /    27 /    45 /   164 | Epsilon:  0.082\n",
      "Episode        5 099 | R (last 100): -84.740 /  -9.820 /  -8.520 /  -6.658 /  -1.670 | Steps:     3 /    16 /    28 /    48 /   215 | Epsilon:  0.078\n",
      "Episode        5 199 | R (last 100): -58.280 /  -9.820 /  -8.375 /  -5.995 /  -1.220 | Steps:     3 /    13 /    29 /    48 /   159 | Epsilon:  0.074\n",
      "Episode        5 299 | R (last 100): -56.850 / -11.695 /  -8.785 /  -6.855 /  -1.010 | Steps:     3 /    16 /    31 /    54 /   186 | Epsilon:  0.071\n",
      "Episode        5 399 | R (last 100): -46.230 /  -9.753 /  -8.520 /  -6.540 /  +0.580 | Steps:     3 /    17 /    30 /    53 /   154 | Epsilon:  0.067\n",
      "Episode        5 499 | R (last 100): -48.530 / -12.230 /  -8.425 /  -6.397 /  +0.120 | Steps:     3 /    21 /    39 /    60 /   184 | Epsilon:  0.064\n",
      "Episode        5 599 | R (last 100): -87.920 / -16.943 /  -9.280 /  -6.300 /  +0.340 | Steps:     3 /    21 /    42 /    78 /   263 | Epsilon:  0.061\n",
      "Episode        5 699 | R (last 100): -167.560 / -16.053 /  -8.835 /  -6.480 /  -2.230 | Steps:     3 /    23 /    42 /    72 /   417 | Epsilon:  0.058\n",
      "Episode        5 799 | R (last 100): -67.930 / -16.750 /  -9.280 /  -7.200 /  -2.280 | Steps:     3 /    21 /    40 /    66 /   207 | Epsilon:  0.055\n",
      "Episode        5 899 | R (last 100): -68.340 / -15.733 /  -9.380 /  -7.840 /  +0.140 | Steps:     3 /    15 /    37 /    88 /   175 | Epsilon:  0.052\n",
      "Episode        5 999 | R (last 100): -72.910 / -15.945 /  -9.775 /  -7.590 /  +0.130 | Steps:     3 /    26 /    44 /    75 /   252 | Epsilon:  0.050\n",
      "Episode        6 099 | R (last 100): -97.260 / -22.235 /  -9.375 /  -7.920 /  +0.390 | Steps:     3 /    18 /    44 /    80 /   337 | Epsilon:  0.047\n",
      "Episode        6 199 | R (last 100): -127.000 / -20.520 /  -9.820 /  -7.885 /  -2.940 | Steps:     3 /    24 /    54 /   101 /   351 | Epsilon:  0.045\n",
      "Episode        6 299 | R (last 100): -82.010 / -23.000 /  -9.595 /  -7.945 /  +1.120 | Steps:     5 /    22 /    50 /    91 /   239 | Epsilon:  0.043\n",
      "Episode        6 399 | R (last 100): -80.210 / -17.515 /  -9.505 /  -7.245 /  +0.970 | Steps:     3 /    21 /    48 /    82 /   232 | Epsilon:  0.041\n",
      "Episode        6 499 | R (last 100): -107.730 / -21.710 /  -9.730 /  -6.867 /  -0.230 | Steps:     3 /    27 /    55 /    96 /   304 | Epsilon:  0.039\n",
      "Episode        6 599 | R (last 100): -148.290 / -21.703 /  -9.380 /  -7.582 /  +0.850 | Steps:     3 /    22 /    51 /    91 /   370 | Epsilon:  0.037\n",
      "Episode        6 699 | R (last 100): -108.960 / -25.693 /  -9.685 /  -7.223 /  +1.790 | Steps:     3 /    33 /    60 /   109 /   307 | Epsilon:  0.035\n",
      "Episode        6 799 | R (last 100): -112.390 / -21.083 /  -9.680 /  -6.818 /  -0.690 | Steps:     3 /    25 /    58 /    96 /   330 | Epsilon:  0.033\n",
      "Episode        6 899 | R (last 100): -114.260 / -19.505 /  -9.675 /  -7.328 /  +1.120 | Steps:     4 /    26 /    50 /    90 /   325 | Epsilon:  0.032\n",
      "Episode        6 999 | R (last 100): -172.510 / -36.715 / -13.180 /  -8.010 /  +1.740 | Steps:     3 /    35 /    73 /   134 /   462 | Epsilon:  0.030\n",
      "Episode        7 099 | R (last 100): -92.590 / -25.998 /  -9.680 /  -6.575 /  -0.060 | Steps:     3 /    31 /    54 /   106 /   300 | Epsilon:  0.030\n",
      "Episode        7 199 | R (last 100): -201.590 / -29.605 / -11.400 /  -6.973 /  +2.870 | Steps:     3 /    32 /    64 /   106 /   500 | Epsilon:  0.030\n",
      "Episode        7 299 | R (last 100): -176.200 / -30.415 /  -9.760 /  -6.998 /  +0.030 | Steps:     5 /    34 /    72 /   113 /   471 | Epsilon:  0.030\n",
      "Episode        7 399 | R (last 100): -214.330 / -24.463 /  -8.380 /  -4.138 /  +0.670 | Steps:     7 /    39 /    62 /   116 /   564 | Epsilon:  0.030\n",
      "Episode        7 499 | R (last 100): -136.060 / -27.725 / -13.910 /  -6.643 /  +1.300 | Steps:     3 /    40 /    80 /   114 /   378 | Epsilon:  0.030\n",
      "Episode        7 599 | R (last 100): -252.430 / -21.893 /  -9.730 /  -6.033 /  -0.040 | Steps:     4 /    33 /    59 /   100 /   624 | Epsilon:  0.030\n",
      "Episode        7 699 | R (last 100): -112.440 / -23.423 / -10.475 /  -6.765 /  -0.600 | Steps:     3 /    33 /    67 /   117 /   335 | Epsilon:  0.030\n",
      "Episode        7 799 | R (last 100): -99.390 / -27.840 /  -8.375 /  -5.138 /  +2.660 | Steps:     4 /    33 /    59 /   110 /   280 | Epsilon:  0.030\n",
      "Episode        7 899 | R (last 100): -79.440 / -25.648 /  -9.000 /  -5.275 /  +0.750 | Steps:     3 /    35 /    58 /    99 /   245 | Epsilon:  0.030\n",
      "Episode        7 999 | R (last 100): -91.300 / -22.883 / -10.030 /  -5.993 /  +3.330 | Steps:     7 /    37 /    68 /   114 /   281 | Epsilon:  0.030\n",
      "Episode        8 099 | R (last 100): -124.930 / -23.350 /  -9.565 /  -5.895 /  +0.770 | Steps:     4 /    36 /    62 /    96 /   324 | Epsilon:  0.030\n",
      "Episode        8 199 | R (last 100): -146.270 / -19.013 /  -8.920 /  -7.088 /  +2.530 | Steps:     3 /    34 /    62 /   101 /   398 | Epsilon:  0.030\n",
      "Episode        8 299 | R (last 100): -96.360 / -21.660 /  -9.615 /  -6.245 /  +1.560 | Steps:     3 /    25 /    62 /   106 /   309 | Epsilon:  0.030\n",
      "Episode        8 399 | R (last 100): -98.240 / -17.190 /  -7.525 /  -5.275 /  +2.760 | Steps:     4 /    30 /    58 /   102 /   282 | Epsilon:  0.030\n",
      "Episode        8 499 | R (last 100): -115.080 / -14.808 /  -8.190 /  -4.865 /  +7.450 | Steps:     4 /    30 /    54 /   103 /   373 | Epsilon:  0.030\n",
      "Episode        8 599 | R (last 100): -80.220 / -22.430 /  -9.245 /  -5.105 /  +2.780 | Steps:     5 /    36 /    56 /   111 /   293 | Epsilon:  0.030\n",
      "Episode        8 699 | R (last 100): -145.000 / -19.623 /  -9.190 /  -5.800 /  +4.010 | Steps:     5 /    33 /    62 /   106 /   401 | Epsilon:  0.030\n",
      "Episode        8 799 | R (last 100): -166.330 / -26.255 /  -9.345 /  -6.368 /  +3.030 | Steps:     4 /    34 /    62 /   106 /   414 | Epsilon:  0.030\n",
      "Episode        8 899 | R (last 100): -70.390 / -19.020 /  -9.370 /  -5.773 /  +2.830 | Steps:     5 /    33 /    62 /   100 /   262 | Epsilon:  0.030\n",
      "Episode        8 999 | R (last 100): -172.450 / -16.863 /  -8.535 /  -5.603 /  +3.640 | Steps:     3 /    28 /    62 /    93 /   496 | Epsilon:  0.030\n",
      "Episode        9 099 | R (last 100): -99.810 / -21.805 /  -9.280 /  -5.908 /  -0.040 | Steps:     3 /    32 /    60 /   110 /   342 | Epsilon:  0.030\n",
      "Episode        9 199 | R (last 100): -165.100 / -24.738 / -10.740 /  -5.615 /  +3.560 | Steps:     6 /    40 /    71 /   124 /   411 | Epsilon:  0.030\n",
      "Episode        9 299 | R (last 100): -189.700 / -26.960 /  -9.640 /  -5.280 /  +0.120 | Steps:    10 /    44 /    70 /   127 /   471 | Epsilon:  0.030\n",
      "Episode        9 399 | R (last 100): -93.820 / -10.773 /  -7.600 /  -4.488 /  +1.580 | Steps:     4 /    27 /    47 /    76 /   303 | Epsilon:  0.030\n",
      "Episode        9 499 | R (last 100): -90.570 / -19.283 /  -8.690 /  -5.163 /  +2.870 | Steps:     3 /    29 /    54 /    93 /   328 | Epsilon:  0.030\n",
      "Episode        9 599 | R (last 100): -138.100 / -17.540 /  -8.980 /  -5.210 /  +4.820 | Steps:     5 /    30 /    54 /   111 /   411 | Epsilon:  0.030\n",
      "Episode        9 699 | R (last 100): -84.840 / -20.065 /  -9.665 /  -6.268 /  +1.660 | Steps:     4 /    40 /    62 /   104 /   275 | Epsilon:  0.030\n",
      "Episode        9 799 | R (last 100): -181.500 / -19.343 /  -8.645 /  -5.795 /  +2.930 | Steps:     5 /    30 /    62 /   103 /   451 | Epsilon:  0.030\n",
      "Episode        9 899 | R (last 100): -118.380 / -28.428 /  -9.675 /  -6.098 /  +2.210 | Steps:     4 /    34 /    68 /   112 /   319 | Epsilon:  0.030\n",
      "Episode        9 999 | R (last 100): -119.100 / -22.523 / -10.940 /  -7.593 /  +1.680 | Steps:     3 /    32 /    61 /   107 /   351 | Epsilon:  0.030\n",
      "Episode       10 099 | R (last 100): -129.010 / -21.805 / -10.220 /  -6.808 /  +0.580 | Steps:     3 /    29 /    57 /   108 /   412 | Epsilon:  0.030\n",
      "Episode       10 199 | R (last 100): -178.630 / -23.403 /  -9.675 /  -6.967 /  +1.130 | Steps:     4 /    32 /    61 /   126 /   444 | Epsilon:  0.030\n",
      "Episode       10 299 | R (last 100): -139.730 / -26.725 /  -9.150 /  -5.618 /  +2.940 | Steps:     5 /    33 /    62 /   110 /   354 | Epsilon:  0.030\n",
      "Episode       10 399 | R (last 100): -214.710 / -35.680 / -12.870 /  -7.215 /  -1.370 | Steps:     4 /    34 /    82 /   129 /   532 | Epsilon:  0.030\n",
      "Episode       10 499 | R (last 100): -219.150 / -31.238 / -10.135 /  -6.640 /  +0.500 | Steps:     3 /    29 /    65 /   120 /   566 | Epsilon:  0.030\n",
      "Episode       10 599 | R (last 100): -166.560 / -18.595 /  -7.695 /  -5.188 /  +3.650 | Steps:     5 /    33 /    56 /    90 /   417 | Epsilon:  0.030\n",
      "Episode       10 699 | R (last 100): -134.220 / -24.230 /  -9.545 /  -5.720 /  +4.920 | Steps:     3 /    38 /    62 /   108 /   343 | Epsilon:  0.030\n",
      "Episode       10 799 | R (last 100): -202.000 / -18.823 /  -7.730 /  -5.000 /  +2.740 | Steps:     3 /    33 /    54 /    98 /   501 | Epsilon:  0.030\n",
      "Episode       10 899 | R (last 100): -65.980 / -16.338 /  -7.325 /  -4.140 /  +2.580 | Steps:     3 /    32 /    56 /    89 /   241 | Epsilon:  0.030\n",
      "Episode       10 999 | R (last 100): -110.580 / -22.323 /  -8.190 /  -5.470 /  +1.500 | Steps:     3 /    28 /    52 /    95 /   339 | Epsilon:  0.030\n",
      "Episode       11 099 | R (last 100): -202.540 / -21.135 /  -8.905 /  -5.868 /  +5.490 | Steps:     5 /    31 /    63 /   107 /   545 | Epsilon:  0.030\n",
      "Episode       11 199 | R (last 100): -136.150 / -26.250 / -10.010 /  -6.388 /  -0.030 | Steps:     4 /    36 /    61 /   114 /   416 | Epsilon:  0.030\n",
      "Episode       11 299 | R (last 100): -116.240 / -25.690 /  -9.385 /  -6.370 /  +1.210 | Steps:     5 /    40 /    61 /   114 /   365 | Epsilon:  0.030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m agent = \u001B[43mtrain_dqn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodelclass\u001B[49m\u001B[43m=\u001B[49m\u001B[43mSnakeV0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboard_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2_000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 98\u001B[39m, in \u001B[36mtrain_dqn\u001B[39m\u001B[34m(modelclass, board_size, env_name, episodes, max_steps)\u001B[39m\n\u001B[32m     95\u001B[39m prev_length = \u001B[38;5;28mint\u001B[39m(get_snake_length(obs, board_size))\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_steps):\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m     action = \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     99\u001B[39m     next_obs, raw_reward, terminated, truncated, _ = env.step(action)\n\u001B[32m    100\u001B[39m     done = terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 26\u001B[39m, in \u001B[36mDQNAgent.act\u001B[39m\u001B[34m(self, state, epsilon)\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m     25\u001B[39m     q_values = \u001B[38;5;28mself\u001B[39m.q_net(state)\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mq_values\u001B[49m\u001B[43m.\u001B[49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T07:32:15.846544633Z",
     "start_time": "2026-01-01T07:32:15.800057291Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a2bdd4917074a878",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T04:40:12.572135665Z",
     "start_time": "2025-12-26T04:38:51.102318720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate_agent(trained_agent, board_size=10, episodes=10)\n",
    "eval_env = gym.make(\"gym_snakegame/SnakeGame-v0\",\n",
    "                    board_size=10,\n",
    "                    n_channel=1,\n",
    "                    n_target=1,\n",
    "                    render_mode=\"human\")\n",
    "evaluate_agent(agent, eval_env, episodes=200)\n",
    "eval_env.close()"
   ],
   "id": "a378bbc63b8e2b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success rate over 200 episodes: -200/200 (-100.0%)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2d114e1aa24b0d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
