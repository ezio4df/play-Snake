{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Snake 10x10",
   "id": "3c29c28ed1582b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:24:58.431867126Z",
     "start_time": "2025-12-31T07:24:56.488990029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import gym_snakegame\n",
    "from scipy.signal import step\n",
    "import pygame\n",
    "import sys\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Pytorch device:\", device)"
   ],
   "id": "89bb63c31604d604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ezio4df/projects/play-Snake/.venv/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test the gymnasium env",
   "id": "50f2a5f052929594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:25:04.234367862Z",
     "start_time": "2025-12-31T07:24:58.433333108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PLAY_MANUAL = True  # set to False if u wanna see random moves\n",
    "env = gym.make(\n",
    "    \"gym_snakegame/SnakeGame-v0\",\n",
    "    board_size=5,\n",
    "    n_channel=1,\n",
    "    n_target=1,\n",
    "    render_mode=\"human\"\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "KEY_MAP = {\n",
    "    pygame.K_UP: 2,\n",
    "    pygame.K_RIGHT: 1,\n",
    "    pygame.K_DOWN: 0,\n",
    "    pygame.K_LEFT: 3\n",
    "}\n",
    "\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "\n",
    "print(\"hit arrows to play! or close window to quit\")\n",
    "if not PLAY_MANUAL:\n",
    "    print(\"auto mode: doing 250 random steps...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        action = None\n",
    "\n",
    "        if PLAY_MANUAL:\n",
    "            # wait for key press\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    env.close()\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key in KEY_MAP:\n",
    "                        action = KEY_MAP[event.key]\n",
    "            if action is None:\n",
    "                continue  # wait till key pressed\n",
    "        else:\n",
    "            if steps >= 250:\n",
    "                break\n",
    "            action = env.action_space.sample()\n",
    "            steps += 1\n",
    "\n",
    "        # step\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        print(f\"step {steps}/100, score: {total_reward} ({reward} | trunc:{truncated} | trmt:{terminated})\")\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"GameOver!  total score: {total_reward}\")\n",
    "            # reset everything\n",
    "            obs, info = env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    env.close()"
   ],
   "id": "99932bac4b3f83ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit arrows to play! or close window to quit\n",
      "step 0/100, score: 0 (0 | trunc:False | trmt:False)\n",
      "step 0/100, score: 1 (1 | trunc:False | trmt:False)\n",
      "step 0/100, score: 1 (0 | trunc:False | trmt:False)\n",
      "step 0/100, score: 0 (-1 | trunc:False | trmt:True)\n",
      "GameOver!  total score: 0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31merror\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     28\u001B[39m action = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m PLAY_MANUAL:\n\u001B[32m     31\u001B[39m     \u001B[38;5;66;03m# wait for key press\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m \u001B[43mpygame\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     33\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m event.type == pygame.QUIT:\n\u001B[32m     34\u001B[39m             env.close()\n",
      "\u001B[31merror\u001B[39m: video system not initialized"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "aa119f5229e6f30b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-31T07:25:06.967248618Z",
     "start_time": "2025-12-31T07:25:06.952763808Z"
    }
   },
   "source": [
    "class SnakeV0(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(SnakeV0, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Replay buffer",
   "id": "5aefcae6fc70c735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:25:07.565736893Z",
     "start_time": "2025-12-31T07:25:07.542726663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "da4d2ff1fb46a7c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent",
   "id": "34b1775410f5e77f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:25:08.196356160Z",
     "start_time": "2025-12-31T07:25:08.162557796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, modelclass, state_size, action_size, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_net = modelclass(state_size, action_size).to(device)\n",
    "        self.target_net = modelclass(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Sync target network\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        # current q-values\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # next q-values frm target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "\n",
    "        # compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network periodically\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ],
   "id": "6c0a1baf669c3bb3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training loop",
   "id": "4fd9685cad4883f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:27:44.898563057Z",
     "start_time": "2025-12-31T07:27:44.833729010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: make a common reward func\n",
    "\n",
    "def train_dqn(modelclass, board_size=10, env_name=\"gym_snakegame/SnakeGame-v0\", episodes=2000, max_steps=100):\n",
    "    env = gym.make(env_name,\n",
    "                   board_size=board_size,\n",
    "                   n_channel=1,\n",
    "                   n_target=1,\n",
    "                   render_mode=None)\n",
    "    state_size = np.prod(env.observation_space.shape)\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(modelclass=modelclass, state_size=state_size, action_size=action_size)\n",
    "\n",
    "    scores = deque(maxlen=100)  # for moving average\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # rewards calculations\n",
    "            if done and reward == 0:      # died without eating\n",
    "                reward = -1.0\n",
    "            # reward = reward - 0.01  # optional shaping to encourage speed\n",
    "\n",
    "            next_state = next_state.flatten()\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores)\n",
    "            print(f\"Episode {episode}, Avg Reward (last 100): {avg_score:.3f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, env\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, episodes=10, max_steps=100):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)  # greedy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # rewards calculations\n",
    "            if terminated: # died\n",
    "                reward = -10.0\n",
    "            if done and reward == 0:      # died without eating\n",
    "                reward = -1.0\n",
    "            # reward = reward - 0.01  # optional shaping to encourage speed\n",
    "\n",
    "            if done:\n",
    "                success += reward\n",
    "                break\n",
    "    print(f\"\\nSuccess rate over {episodes} episodes: {success}/{episodes} ({100 * success / episodes:.1f}%)\")"
   ],
   "id": "10c397569f5c1c4c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T07:31:03.680032428Z",
     "start_time": "2025-12-31T07:27:45.410543015Z"
    }
   },
   "cell_type": "code",
   "source": "agent, env = train_dqn(modelclass=SnakeV0, episodes=20_000)",
   "id": "f784dd61c66c9c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 0, Avg Reward (last 100): -1.000, Epsilon: 0.995\n",
      "Episode 100, Avg Reward (last 100): -0.880, Epsilon: 0.603\n",
      "Episode 200, Avg Reward (last 100): -0.800, Epsilon: 0.365\n",
      "Episode 300, Avg Reward (last 100): -0.900, Epsilon: 0.221\n",
      "Episode 400, Avg Reward (last 100): -0.910, Epsilon: 0.134\n",
      "Episode 500, Avg Reward (last 100): -0.910, Epsilon: 0.081\n",
      "Episode 600, Avg Reward (last 100): -0.950, Epsilon: 0.049\n",
      "Episode 700, Avg Reward (last 100): -0.940, Epsilon: 0.030\n",
      "Episode 800, Avg Reward (last 100): -0.920, Epsilon: 0.018\n",
      "Episode 900, Avg Reward (last 100): -0.930, Epsilon: 0.011\n",
      "Episode 1000, Avg Reward (last 100): -0.950, Epsilon: 0.010\n",
      "Episode 1100, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 1200, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 1300, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 1400, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 1500, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 1600, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 1700, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 1800, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 1900, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 2000, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 2100, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 2200, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 2300, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 2400, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 2500, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 2600, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 2700, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 2800, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 2900, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 3000, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 3100, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 3200, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 3300, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 3400, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 3500, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 3600, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 3700, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 3800, Avg Reward (last 100): -0.950, Epsilon: 0.010\n",
      "Episode 3900, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 4000, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 4100, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 4200, Avg Reward (last 100): -0.800, Epsilon: 0.010\n",
      "Episode 4300, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 4400, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 4500, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 4600, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 4700, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 4800, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 4900, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 5000, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 5100, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 5200, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 5300, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 5400, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 5500, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 5600, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 5700, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 5800, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 5900, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 6000, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 6100, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 6200, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 6300, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 6400, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 6500, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 6600, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 6700, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 6800, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 6900, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 7000, Avg Reward (last 100): -0.800, Epsilon: 0.010\n",
      "Episode 7100, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 7200, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 7300, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 7400, Avg Reward (last 100): -0.780, Epsilon: 0.010\n",
      "Episode 7500, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 7600, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 7700, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 7800, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 7900, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 8000, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 8100, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 8200, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 8300, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 8400, Avg Reward (last 100): -0.940, Epsilon: 0.010\n",
      "Episode 8500, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 8600, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 8700, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 8800, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 8900, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 9000, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 9100, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 9200, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 9300, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 9400, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 9500, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 9600, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 9700, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 9800, Avg Reward (last 100): -0.800, Epsilon: 0.010\n",
      "Episode 9900, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 10000, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 10100, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 10200, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 10300, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 10400, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 10500, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 10600, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 10700, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 10800, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 10900, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 11000, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 11100, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 11200, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 11300, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 11400, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 11500, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 11600, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 11700, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 11800, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 11900, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 12000, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 12100, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 12200, Avg Reward (last 100): -0.790, Epsilon: 0.010\n",
      "Episode 12300, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 12400, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 12500, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 12600, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 12700, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 12800, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 12900, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 13000, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 13100, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 13200, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 13300, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 13400, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 13500, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 13600, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 13700, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 13800, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 13900, Avg Reward (last 100): -0.790, Epsilon: 0.010\n",
      "Episode 14000, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 14100, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 14200, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 14300, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 14400, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 14500, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 14600, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 14700, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 14800, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 14900, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 15000, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 15100, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 15200, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 15300, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 15400, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 15500, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 15600, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 15700, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 15800, Avg Reward (last 100): -0.770, Epsilon: 0.010\n",
      "Episode 15900, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 16000, Avg Reward (last 100): -0.800, Epsilon: 0.010\n",
      "Episode 16100, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 16200, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 16300, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 16400, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 16500, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 16600, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 16700, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 16800, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 16900, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 17000, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 17100, Avg Reward (last 100): -0.920, Epsilon: 0.010\n",
      "Episode 17200, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 17300, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 17400, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 17500, Avg Reward (last 100): -0.840, Epsilon: 0.010\n",
      "Episode 17600, Avg Reward (last 100): -0.790, Epsilon: 0.010\n",
      "Episode 17700, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 17800, Avg Reward (last 100): -0.910, Epsilon: 0.010\n",
      "Episode 17900, Avg Reward (last 100): -0.800, Epsilon: 0.010\n",
      "Episode 18000, Avg Reward (last 100): -0.890, Epsilon: 0.010\n",
      "Episode 18100, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 18200, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 18300, Avg Reward (last 100): -0.870, Epsilon: 0.010\n",
      "Episode 18400, Avg Reward (last 100): -0.880, Epsilon: 0.010\n",
      "Episode 18500, Avg Reward (last 100): -0.940, Epsilon: 0.010\n",
      "Episode 18600, Avg Reward (last 100): -0.790, Epsilon: 0.010\n",
      "Episode 18700, Avg Reward (last 100): -0.830, Epsilon: 0.010\n",
      "Episode 18800, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 18900, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 19000, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 19100, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 19200, Avg Reward (last 100): -0.820, Epsilon: 0.010\n",
      "Episode 19300, Avg Reward (last 100): -0.850, Epsilon: 0.010\n",
      "Episode 19400, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 19500, Avg Reward (last 100): -0.860, Epsilon: 0.010\n",
      "Episode 19600, Avg Reward (last 100): -0.900, Epsilon: 0.010\n",
      "Episode 19700, Avg Reward (last 100): -0.810, Epsilon: 0.010\n",
      "Episode 19800, Avg Reward (last 100): -0.930, Epsilon: 0.010\n",
      "Episode 19900, Avg Reward (last 100): -0.810, Epsilon: 0.010\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T04:40:12.572135665Z",
     "start_time": "2025-12-26T04:38:51.102318720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_env = gym.make(\"gym_snakegame/SnakeGame-v0\",\n",
    "                    board_size=10,\n",
    "                    n_channel=1,\n",
    "                    n_target=1,\n",
    "                    render_mode=\"human\")\n",
    "evaluate_agent(agent, eval_env, episodes=200)\n",
    "eval_env.close()"
   ],
   "id": "a378bbc63b8e2b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success rate over 200 episodes: -200/200 (-100.0%)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9bf563212baca305"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# .astype(np.float32)\n",
    "\n"
   ],
   "id": "bfcf065f8753e74c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "# TODO: make a common reward func\n",
    "\n",
    "def train_dqn(modelclass, board_size=10, env_name=\"gym_snakegame/SnakeGame-v0\", episodes=2000, max_steps=100):\n",
    "    env = gym.make(env_name,\n",
    "                   board_size=board_size,\n",
    "                   n_channel=1,\n",
    "                   n_target=1,\n",
    "                   render_mode=None)\n",
    "    state_size = np.prod(env.observation_space.shape)\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(modelclass=modelclass, state_size=state_size, action_size=action_size)\n",
    "\n",
    "    scores = deque(maxlen=100)  # for moving average\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # rewards calculations\n",
    "            if done and reward == 0:      # died without eating\n",
    "                reward = -1.0\n",
    "            # reward = reward - 0.01  # optional shaping to encourage speed\n",
    "\n",
    "            next_state = next_state.flatten()\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores)\n",
    "            print(f\"Episode {episode}, Avg Reward (last 100): {avg_score:.3f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "    env.close()\n",
    "    return agent, env\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, episodes=10, max_steps=100):\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.flatten()\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)  # greedy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = state.flatten()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # rewards calculations\n",
    "            if terminated: # died\n",
    "                reward = -10.0\n",
    "            if done and reward == 0:      # died without eating\n",
    "                reward = -1.0\n",
    "            # reward = reward - 0.01  # optional shaping to encourage speed\n",
    "\n",
    "            if done:\n",
    "                success += reward\n",
    "                break\n",
    "    print(f\"\\nSuccess rate over {episodes} episodes: {success}/{episodes} ({100 * success / episodes:.1f}%)\")\n",
    "```\n",
    "\n",
    "make a common reward function asnd use it in both places\n",
    "make the reward function such that\n",
    "- when the snake body filles the whole board -> reward very high (as it filled the whole board it must have eaten all items so i dont think theres need to check score)\n",
    "- when snake hits wall -> very heavy punishment\n",
    "- when movign slow -> punishment\n",
    "- when in loop and does nothing -> mid punishment\n",
    "- when eating item -> (low/mid/high suggest which ones best) reward\n",
    "-"
   ],
   "id": "2590c3a74faab59d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:14:06.174785530Z",
     "start_time": "2025-12-26T11:14:04.070232153Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ebfea1923b045970",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up with batch_size=1: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! No positional inputs found for a module, assuming batch size is 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up with batch_size=1: 100%|██████████| 10/10 [00:00<00:00, 430.66it/s]\n",
      "Measuring inference for batch_size=1: 100%|██████████| 100/100 [00:00<00:00, 456.90it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n",
      "Warming up with batch_size=8: 100%|██████████| 10/10 [00:00<00:00, 194.06it/s]\n",
      "Measuring inference for batch_size=8: 100%|██████████| 100/100 [00:00<00:00, 185.59it/s]\n",
      "Unable to measure energy consumption. Device must be a NVIDIA Jetson.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T11:14:09.737262635Z",
     "start_time": "2025-12-26T11:14:09.709433594Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "d57655b8891b077e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine_info': {'system': {'system': 'Linux',\n",
       "   'node': 'katana',\n",
       "   'release': '6.12.63-1-MANJARO'},\n",
       "  'cpu': {'model': 'Intel(R) Core(TM) i7-14650HX',\n",
       "   'architecture': 'x86_64',\n",
       "   'cores': {'physical': 16, 'total': 24},\n",
       "   'frequency': '4.60 GHz'},\n",
       "  'memory': {'total': '15.36 GB', 'used': '7.89 GB', 'available': '7.47 GB'},\n",
       "  'gpus': [{'name': 'NVIDIA GeForce RTX 5050 Laptop GPU',\n",
       "    'memory': '8151.0 MB'}]},\n",
       " 'device': 'cuda',\n",
       " 'params': 5288548,\n",
       " 'flops': 401669732,\n",
       " 'timing': {'batch_size_1': {'on_device_inference': {'metrics': {'batches_per_second_mean': -0.48618864687214386,\n",
       "     'batches_per_second_std': 0.037266361886206814,\n",
       "     'batches_per_second_min': -0.5373293528065449,\n",
       "     'batches_per_second_max': -0.3431878548124394,\n",
       "     'seconds_per_batch_mean': -2.0705615985393524,\n",
       "     'seconds_per_batch_std': 0.18086291895638007,\n",
       "     'seconds_per_batch_min': -2.913856029510498,\n",
       "     'seconds_per_batch_max': -1.8610559701919556},\n",
       "    'human_readable': {'batches_per_second': '-0.49 +/- 0.04 [-0.54, -0.34]',\n",
       "     'batch_latency': '-2070561.599 us +/- 180.863 ms [-2913856.030 us, -1861055.970 us]'}},\n",
       "   'cpu_to_gpu': {'metrics': {'batches_per_second_mean': 14673.610916901029,\n",
       "     'batches_per_second_std': 2666.6101041216416,\n",
       "     'batches_per_second_min': 1032.0629921259842,\n",
       "     'batches_per_second_max': 17924.37606837607,\n",
       "     'seconds_per_batch_mean': 7.861375808715821e-05,\n",
       "     'seconds_per_batch_std': 9.057771817231792e-05,\n",
       "     'seconds_per_batch_min': 5.5789947509765625e-05,\n",
       "     'seconds_per_batch_max': 0.00096893310546875},\n",
       "    'human_readable': {'batches_per_second': '14.67 K +/- 2.67 K [1.03 K, 17.92 K]',\n",
       "     'batch_latency': '78.614 us +/- 90.578 us [55.790 us, 968.933 us]'}},\n",
       "   'gpu_to_cpu': {'metrics': {'batches_per_second_mean': 55798.775773582376,\n",
       "     'batches_per_second_std': 8391.15707971617,\n",
       "     'batches_per_second_min': 34379.54098360656,\n",
       "     'batches_per_second_max': 67650.06451612903,\n",
       "     'seconds_per_batch_mean': 1.8410682678222656e-05,\n",
       "     'seconds_per_batch_std': 3.313688906764252e-06,\n",
       "     'seconds_per_batch_min': 1.4781951904296875e-05,\n",
       "     'seconds_per_batch_max': 2.9087066650390625e-05},\n",
       "    'human_readable': {'batches_per_second': '55.80 K +/- 8.39 K [34.38 K, 67.65 K]',\n",
       "     'batch_latency': '18.411 us +/- 3.314 us [14.782 us, 29.087 us]'}},\n",
       "   'total': {'metrics': {'batches_per_second_mean': 463.8078287013521,\n",
       "     'batches_per_second_std': 40.29135577128504,\n",
       "     'batches_per_second_min': 288.6651066758431,\n",
       "     'batches_per_second_max': 515.3340705246345,\n",
       "     'seconds_per_batch_mean': 0.002175900936126709,\n",
       "     'seconds_per_batch_std': 0.0002320503593036113,\n",
       "     'seconds_per_batch_min': 0.0019404888153076172,\n",
       "     'seconds_per_batch_max': 0.003464221954345703},\n",
       "    'human_readable': {'batches_per_second': '463.81 +/- 40.29 [288.67, 515.33]',\n",
       "     'batch_latency': '2.176 ms +/- 232.050 us [1.940 ms, 3.464 ms]'}}},\n",
       "  'batch_size_8': {'on_device_inference': {'metrics': {'batches_per_second_mean': -0.20307278483713223,\n",
       "     'batches_per_second_std': 0.001228571813610911,\n",
       "     'batches_per_second_min': -0.2052760841525918,\n",
       "     'batches_per_second_max': -0.19919938479682187,\n",
       "     'seconds_per_batch_mean': -4.92452383518219,\n",
       "     'seconds_per_batch_std': 0.029929503425201,\n",
       "     'seconds_per_batch_min': -5.0200958251953125,\n",
       "     'seconds_per_batch_max': -4.871488094329834},\n",
       "    'human_readable': {'batches_per_second': '-0.20 +/- 0.00 [-0.21, -0.20]',\n",
       "     'batch_latency': '-4924523.835 us +/- 29.930 ms [-5020095.825 us, -4871488.094 us]'}},\n",
       "   'cpu_to_gpu': {'metrics': {'batches_per_second_mean': 2733.635797311106,\n",
       "     'batches_per_second_std': 690.5986254863772,\n",
       "     'batches_per_second_min': 1492.1038776236214,\n",
       "     'batches_per_second_max': 3964.3705103969755,\n",
       "     'seconds_per_batch_mean': 0.00038950443267822267,\n",
       "     'seconds_per_batch_std': 9.706963207765218e-05,\n",
       "     'seconds_per_batch_min': 0.0002522468566894531,\n",
       "     'seconds_per_batch_max': 0.0006701946258544922},\n",
       "    'human_readable': {'batches_per_second': '2.73 K +/- 690.60 [1.49 K, 3.96 K]',\n",
       "     'batch_latency': '389.504 us +/- 97.070 us [252.247 us, 670.195 us]'}},\n",
       "   'gpu_to_cpu': {'metrics': {'batches_per_second_mean': 31354.418464496055,\n",
       "     'batches_per_second_std': 10098.99919170295,\n",
       "     'batches_per_second_min': 10922.666666666666,\n",
       "     'batches_per_second_max': 50533.78313253012,\n",
       "     'seconds_per_batch_mean': 3.569841384887695e-05,\n",
       "     'seconds_per_batch_std': 1.2906390922801554e-05,\n",
       "     'seconds_per_batch_min': 1.9788742065429688e-05,\n",
       "     'seconds_per_batch_max': 9.1552734375e-05},\n",
       "    'human_readable': {'batches_per_second': '31.35 K +/- 10.10 K [10.92 K, 50.53 K]',\n",
       "     'batch_latency': '35.698 us +/- 12.906 us [19.789 us, 91.553 us]'}},\n",
       "   'total': {'metrics': {'batches_per_second_mean': 186.68005633892898,\n",
       "     'batches_per_second_std': 4.00275323207447,\n",
       "     'batches_per_second_min': 175.22262606007436,\n",
       "     'batches_per_second_max': 192.90364715080716,\n",
       "     'seconds_per_batch_mean': 0.0053592538833618164,\n",
       "     'seconds_per_batch_std': 0.0001164210114580249,\n",
       "     'seconds_per_batch_min': 0.0051839351654052734,\n",
       "     'seconds_per_batch_max': 0.0057070255279541016},\n",
       "    'human_readable': {'batches_per_second': '186.68 +/- 4.00 [175.22, 192.90]',\n",
       "     'batch_latency': '5.359 ms +/- 116.421 us [5.184 ms, 5.707 ms]'}}}},\n",
       " 'memory': {'batch_size_1': {'pre_inference_bytes': 30970368,\n",
       "   'max_inference_bytes': 42009088,\n",
       "   'post_inference_bytes': 30970368,\n",
       "   'pre_inference': '29.54 MB',\n",
       "   'max_inference': '40.06 MB',\n",
       "   'post_inference': '29.54 MB'},\n",
       "  'batch_size_8': {'pre_inference_bytes': 30970368,\n",
       "   'max_inference_bytes': 119280128,\n",
       "   'post_inference_bytes': 30970368,\n",
       "   'pre_inference': '29.54 MB',\n",
       "   'max_inference': '113.75 MB',\n",
       "   'post_inference': '29.54 MB'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
